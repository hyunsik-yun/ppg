{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8800236-854d-42f6-8b02-2e6da4cd8abd",
   "metadata": {},
   "source": [
    "# 1. train/test 나눠서 한거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e786e79-0093-41de-a6bc-1baa5ce7ccb3",
   "metadata": {},
   "source": [
    "## 1. train from \"DROPEDGE\" - about 1000 train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5a8a14-878d-48f4-8047-e7dc3bfce43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f3043-b42b-4ac3-b19f-025c3ccc0418",
   "metadata": {},
   "source": [
    "### 이거는 저장해두자. 어떻게 test_mask 안 겹치게 뽑는지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99208101-76bf-4357-b45e-3422eb4231b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/', name='Cora')\n",
    "\n",
    "# 데이터 확인\n",
    "data = dataset[0]  # Cora 데이터셋은 하나의 그래프 데이터로 구성됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2478168-3478-426c-b40d-693fcca299a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = data.test_mask\n",
    "test_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9bb3cdd-1979-4647-912f-6bfcc91f4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1200\n",
      "Overlap with test set: 0\n"
     ]
    }
   ],
   "source": [
    "test_mask = data.test_mask\n",
    "\n",
    "# 3. test_mask가 False인 노드들(= non-test 노드)만 골라낸 인덱스\n",
    "non_test_indices = torch.where(~test_mask)[0]  # ~test_mask: test_mask가 False인 노드\n",
    "\n",
    "# 4. non_test_indices 중에서 랜덤하게 1000개 선택\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1200]]\n",
    "\n",
    "# 5. 새로운 train_mask 생성\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "\n",
    "# 6. data.train_mask에 할당\n",
    "data.train_mask = new_train_mask\n",
    "\n",
    "# 이제 data.train_mask에는 test set과 겹치지 않는 1000개의 노드만 True로 설정됨\n",
    "print(\"Train set size:\", data.train_mask.sum().item())\n",
    "print(\"Overlap with test set:\", (data.train_mask & data.test_mask).sum().item())  # 0이어야 정상\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ff1d112-4e0f-4107-862e-338b21bd3358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c234d372-bfc4-4476-9e1f-b48551dab6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2354475/1954964068.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.2: Loss = 1.9479\n",
      "Time 0.3: Loss = 1.8857\n",
      "Time 0.4: Loss = 1.7712\n",
      "Time 0.5: Loss = 1.7585\n",
      "Time 0.6: Loss = 1.5070\n",
      "Time 0.7: Loss = 1.5719\n",
      "Time 0.8: Loss = 1.5061\n",
      "Time 0.9: Loss = 1.3534\n",
      "Time 1.0: Loss = 1.3661\n",
      "Time 1.1: Loss = 1.4134\n",
      "Time 1.2: Loss = 1.2804\n",
      "Time 1.3: Loss = 1.2186\n",
      "Time 1.4: Loss = 1.1115\n",
      "Time 1.5: Loss = 1.0350\n",
      "Time 1.6: Loss = 1.0409\n",
      "Time 1.7: Loss = 0.8810\n",
      "Time 1.8: Loss = 0.9083\n",
      "Time 1.9: Loss = 0.7292\n",
      "Time 2.0: Loss = 0.7991\n",
      "Time 2.1: Loss = 0.8788\n",
      "Time 2.2: Loss = 0.7259\n",
      "Time 2.3: Loss = 0.7311\n",
      "Time 2.4: Loss = 0.5960\n",
      "Time 2.5: Loss = 0.6093\n",
      "Time 2.6: Loss = 0.5774\n",
      "Time 2.7: Loss = 0.5463\n",
      "Time 2.8: Loss = 0.5751\n",
      "Time 2.9: Loss = 0.4810\n",
      "Time 3.0: Loss = 0.3837\n",
      "Time 3.1: Loss = 0.4277\n",
      "Time 3.2: Loss = 0.4005\n",
      "Time 3.3: Loss = 0.3946\n",
      "Time 3.4: Loss = 0.3397\n",
      "Time 3.5: Loss = 0.4733\n",
      "Time 3.6: Loss = 0.3217\n",
      "Time 3.7: Loss = 0.3399\n",
      "Time 3.8: Loss = 0.3028\n",
      "Time 3.9: Loss = 0.4294\n",
      "Time 4.0: Loss = 0.2548\n",
      "Time 4.1: Loss = 0.3524\n",
      "Time 4.2: Loss = 0.2278\n",
      "Time 4.3: Loss = 0.2988\n",
      "Time 4.4: Loss = 0.1886\n",
      "Time 4.5: Loss = 0.3954\n",
      "Time 4.6: Loss = 0.3073\n",
      "Time 4.7: Loss = 0.2616\n",
      "Time 4.8: Loss = 0.2281\n",
      "Time 4.9: Loss = 0.3515\n",
      "Time 5.0: Loss = 0.1834\n",
      "Time 5.1: Loss = 0.2842\n",
      "Time 5.2: Loss = 0.3385\n",
      "Time 5.3: Loss = 0.2512\n",
      "Time 5.4: Loss = 0.2586\n",
      "Time 5.5: Loss = 0.1377\n",
      "Time 5.6: Loss = 0.2017\n",
      "Time 5.7: Loss = 0.1520\n",
      "Time 5.8: Loss = 0.2710\n",
      "Time 5.9: Loss = 0.3301\n",
      "Time 6.0: Loss = 0.2141\n",
      "Time 6.1: Loss = 0.2916\n",
      "Time 6.2: Loss = 0.2739\n",
      "Time 6.3: Loss = 0.2933\n",
      "Time 6.4: Loss = 0.1479\n",
      "Time 6.5: Loss = 0.2086\n",
      "Time 6.6: Loss = 0.3746\n",
      "Time 6.7: Loss = 0.1011\n",
      "Time 6.8: Loss = 0.2753\n",
      "Time 6.9: Loss = 0.2206\n",
      "Time 7.0: Loss = 0.3621\n",
      "Time 7.1: Loss = 0.3398\n",
      "Time 7.2: Loss = 0.2642\n",
      "Time 7.3: Loss = 0.1956\n",
      "Time 7.4: Loss = 0.1996\n",
      "Time 7.5: Loss = 0.4210\n",
      "Time 7.6: Loss = 0.1608\n",
      "Time 7.7: Loss = 0.2322\n",
      "Time 7.8: Loss = 0.1620\n",
      "Time 7.9: Loss = 0.1694\n",
      "Time 8.0: Loss = 0.1784\n",
      "Time 8.1: Loss = 0.1238\n",
      "Time 8.2: Loss = 0.1776\n",
      "Time 8.3: Loss = 0.1777\n",
      "Time 8.4: Loss = 0.1269\n",
      "Time 8.5: Loss = 0.1728\n",
      "Time 8.6: Loss = 0.1491\n",
      "Time 8.7: Loss = 0.2137\n",
      "Time 8.8: Loss = 0.1794\n",
      "Time 8.9: Loss = 0.1352\n",
      "Time 9.0: Loss = 0.1090\n",
      "Time 9.1: Loss = 0.3076\n",
      "Time 9.2: Loss = 0.1337\n",
      "Time 9.3: Loss = 0.1586\n",
      "Time 9.4: Loss = 0.1640\n",
      "Time 9.5: Loss = 0.1716\n",
      "Time 9.6: Loss = 0.1131\n",
      "Time 9.7: Loss = 0.2118\n",
      "Time 9.8: Loss = 0.1525\n",
      "Time 9.9: Loss = 0.0719\n",
      "Time 10.0: Loss = 0.1604\n",
      "Time 10.1: Loss = 0.1354\n",
      "Time 10.2: Loss = 0.0965\n",
      "Time 10.3: Loss = 0.1818\n",
      "Time 10.4: Loss = 0.1850\n",
      "Time 10.5: Loss = 0.0921\n",
      "Time 10.6: Loss = 0.1273\n",
      "Time 10.7: Loss = 0.1301\n",
      "Time 10.8: Loss = 0.1495\n",
      "Time 10.9: Loss = 0.2646\n",
      "Time 11.0: Loss = 0.0965\n",
      "Time 11.1: Loss = 0.1342\n",
      "Time 11.2: Loss = 0.0958\n",
      "Time 11.3: Loss = 0.0924\n",
      "Time 11.4: Loss = 0.1007\n",
      "Time 11.5: Loss = 0.1035\n",
      "Time 11.6: Loss = 0.0635\n",
      "Time 11.7: Loss = 0.0846\n",
      "Time 11.8: Loss = 0.1204\n",
      "Time 11.9: Loss = 0.0917\n",
      "Time 12.0: Loss = 0.0935\n",
      "Time 12.1: Loss = 0.0795\n",
      "Time 12.2: Loss = 0.2043\n",
      "Time 12.3: Loss = 0.2139\n",
      "Time 12.4: Loss = 0.1004\n",
      "Time 12.5: Loss = 0.1052\n",
      "Time 12.6: Loss = 0.1109\n",
      "Time 12.7: Loss = 0.1687\n",
      "Time 12.8: Loss = 0.0710\n",
      "Time 12.9: Loss = 0.0815\n",
      "Time 13.0: Loss = 0.1300\n",
      "Time 13.1: Loss = 0.1568\n",
      "Time 13.2: Loss = 0.0726\n",
      "Time 13.3: Loss = 0.0830\n",
      "Time 13.4: Loss = 0.0562\n",
      "Time 13.5: Loss = 0.1043\n",
      "Time 13.6: Loss = 0.1392\n",
      "Time 13.7: Loss = 0.1195\n",
      "Time 13.8: Loss = 0.1898\n",
      "Time 13.9: Loss = 0.0877\n",
      "Time 14.0: Loss = 0.0789\n",
      "Time 14.1: Loss = 0.1540\n",
      "Time 14.2: Loss = 0.0915\n",
      "Time 14.3: Loss = 0.1028\n",
      "Time 14.4: Loss = 0.0771\n",
      "Time 14.5: Loss = 0.1161\n",
      "Time 14.6: Loss = 0.0662\n",
      "Time 14.7: Loss = 0.1732\n",
      "Time 14.8: Loss = 0.0825\n",
      "Time 14.9: Loss = 0.0838\n",
      "Time 15.0: Loss = 0.0903\n",
      "Time 15.1: Loss = 0.1371\n",
      "Time 15.2: Loss = 0.0584\n",
      "Time 15.3: Loss = 0.0634\n",
      "Time 15.4: Loss = 0.0508\n",
      "Time 15.5: Loss = 0.1384\n",
      "Time 15.6: Loss = 0.1879\n",
      "Time 15.7: Loss = 0.0470\n",
      "Time 15.8: Loss = 0.0836\n",
      "Time 15.9: Loss = 0.0621\n",
      "Time 16.0: Loss = 0.0773\n",
      "Time 16.1: Loss = 0.1110\n",
      "Time 16.2: Loss = 0.1011\n",
      "Time 16.3: Loss = 0.1990\n",
      "Time 16.4: Loss = 0.1904\n",
      "Time 16.5: Loss = 0.1415\n",
      "Time 16.6: Loss = 0.0723\n",
      "Time 16.7: Loss = 0.1018\n",
      "Time 16.8: Loss = 0.0646\n",
      "Time 16.9: Loss = 0.1369\n",
      "Time 17.0: Loss = 0.0675\n",
      "Time 17.1: Loss = 0.1148\n",
      "Time 17.2: Loss = 0.0910\n",
      "Time 17.3: Loss = 0.0322\n",
      "Time 17.4: Loss = 0.0404\n",
      "Time 17.5: Loss = 0.0501\n",
      "Time 17.6: Loss = 0.0742\n",
      "Time 17.7: Loss = 0.1791\n",
      "Time 17.8: Loss = 0.1502\n",
      "Time 17.9: Loss = 0.1184\n",
      "Time 18.0: Loss = 0.0774\n",
      "Time 18.1: Loss = 0.0539\n",
      "Time 18.2: Loss = 0.0716\n",
      "Time 18.3: Loss = 0.0902\n",
      "Time 18.4: Loss = 0.1242\n",
      "Time 18.5: Loss = 0.0722\n",
      "Time 18.6: Loss = 0.1384\n",
      "Time 18.7: Loss = 0.0908\n",
      "Time 18.8: Loss = 0.1552\n",
      "Time 18.9: Loss = 0.0979\n",
      "Time 19.0: Loss = 0.0808\n",
      "Time 19.1: Loss = 0.0961\n",
      "Time 19.2: Loss = 0.1370\n",
      "Time 19.3: Loss = 0.1450\n",
      "Time 19.4: Loss = 0.1151\n",
      "Time 19.5: Loss = 0.0487\n",
      "Time 19.6: Loss = 0.0636\n",
      "Time 19.7: Loss = 0.1917\n",
      "Time 19.8: Loss = 0.1371\n",
      "Time 19.9: Loss = 0.0360\n",
      "Time 20.0: Loss = 0.0877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 확인\n",
    "#data = dataset[0]  # Cora 데이터셋은 하나의 그래프 데이터로 구성됨\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        # 선택된 노드만 반환\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x  \n",
    "\n",
    "# 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Poisson 시뮬레이션 + 학습\n",
    "N = data.num_nodes  # 노드 개수\n",
    "lambda_ = 0.5  # Poisson rate\n",
    "T = 20  # 총 시뮬레이션 시간\n",
    "\n",
    "# 노드별 첫 번째 이벤트 발생 시간 (지수 분포 샘플링)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]  # 각 노드의 이벤트 발생 시간 기록\n",
    "\n",
    "# 훈련 데이터 마스크 가져오기\n",
    "train_mask = data.train_mask.cpu().numpy()\n",
    "\n",
    "# **Training 함수 (선택된 노드만 학습)**\n",
    "def train(selected_nodes):\n",
    "    if len(selected_nodes) == 0:  # 선택된 노드가 없으면 학습 건너뛰기\n",
    "        return None\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)  # 선택된 노드만 예측\n",
    "    labels = data.y[selected_nodes].to(device)  # 선택된 노드들의 레이블만 사용\n",
    "\n",
    "    loss = criterion(out, labels)  # 선택된 노드들에 대해서만 loss 계산\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# **시뮬레이션 실행 + 학습**\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []  # 해당 노드들만 Convolution 하기 위해 index 선택\n",
    "    for i in range(N):\n",
    "        if t >= event_times[i] and train_mask[i]:  # Poisson clock이 울리고 train_mask가 True인 경우만 선택\n",
    "            selected_nodes.append(i)  # 해당 노드들만 선택\n",
    "            event_logs[i].append(t)  # 발생 시간 저장\n",
    "            event_times[i] += np.random.exponential(1/lambda_)  # 다음 이벤트 시간 설정\n",
    "            \n",
    "    t += 0.1  # 작은 타임스텝으로 증가\n",
    "    if selected_nodes:\n",
    "        selected = torch.tensor(selected_nodes, dtype=torch.long, device=device)\n",
    "        loss = train(selected)\n",
    "        if loss is not None:\n",
    "            print(f\"Time {t:.1f}: Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bcfe020-49cf-43e6-8be0-c661df97f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8510\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))  # 모든 노드에 대한 예측\n",
    "        pred = out.argmax(dim=1)  # 가장 높은 확률을 가진 클래스를 예측\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  # 정확히 맞춘 개수\n",
    "        acc = correct / data.test_mask.sum().item()  # 정확도 계산\n",
    "    return acc\n",
    "\n",
    "# 🔹 테스트 실행\n",
    "test_acc = test()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31b0151f-70b8-4e6d-8658-f7b100ef0d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Training set contains test nodes!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(data\u001b[38;5;241m.\u001b[39mtest_mask[train_indices]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set contains test nodes!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Training set contains test nodes!"
     ]
    }
   ],
   "source": [
    "assert not any(data.test_mask[train_indices]), \"Training set contains test nodes!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136e99d8-b13a-43ea-8ba6-1968e575ef51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d45d7de0-0668-4b9b-8787-142553a6e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "\n",
    "# 데이터 확인\n",
    "data = dataset[0]  # Cora 데이터셋은 하나의 그래프 데이터로 구성됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79f622be-f51e-4a26-a7a6-76b7bc97875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1800\n",
      "Overlap with test set: 0\n"
     ]
    }
   ],
   "source": [
    "test_mask = data.test_mask\n",
    "\n",
    "# 3. test_mask가 False인 노드들(= non-test 노드)만 골라낸 인덱스\n",
    "non_test_indices = torch.where(~test_mask)[0]  # ~test_mask: test_mask가 False인 노드\n",
    "\n",
    "# 4. non_test_indices 중에서 랜덤하게 1000개 선택\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "# 5. 새로운 train_mask 생성\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "\n",
    "# 6. data.train_mask에 할당\n",
    "data.train_mask = new_train_mask\n",
    "\n",
    "# 이제 data.train_mask에는 test set과 겹치지 않는 1000개의 노드만 True로 설정됨\n",
    "print(\"Train set size:\", data.train_mask.sum().item())\n",
    "print(\"Overlap with test set:\", (data.train_mask & data.test_mask).sum().item())  # 0이어야 정상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e194f6c-2b33-444c-bf36-084b9c08fb40",
   "metadata": {},
   "source": [
    "## 이게 진짜임. train_mask 설정시켜놓고(1200개) poisson process로 시뮬레이션 학습 후 200 epoch 돌리기.(Citeseer with 1800개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1621d5-0f5e-460d-adac-8a1b62d4bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1800\n",
      "Overlap with test set: 0\n",
      "[Time 0.2] Poisson Loss: 1.7926\n",
      "[Time 0.3] Poisson Loss: 1.7413\n",
      "[Time 0.4] Poisson Loss: 1.6850\n",
      "[Time 0.5] Poisson Loss: 1.5921\n",
      "[Time 0.6] Poisson Loss: 1.4736\n",
      "[Time 0.7] Poisson Loss: 1.4071\n",
      "[Time 0.8] Poisson Loss: 1.4375\n",
      "[Time 0.9] Poisson Loss: 1.3083\n",
      "[Time 1.0] Poisson Loss: 1.3243\n",
      "[Time 1.1] Poisson Loss: 1.1727\n",
      "[Time 1.2] Poisson Loss: 1.1228\n",
      "[Time 1.3] Poisson Loss: 1.1177\n",
      "[Time 1.4] Poisson Loss: 1.0256\n",
      "[Time 1.5] Poisson Loss: 1.1375\n",
      "[Time 1.6] Poisson Loss: 1.0177\n",
      "[Time 1.7] Poisson Loss: 0.9198\n",
      "[Time 1.8] Poisson Loss: 0.8954\n",
      "[Time 1.9] Poisson Loss: 0.8644\n",
      "[Time 2.0] Poisson Loss: 0.9075\n",
      "[Time 2.1] Poisson Loss: 0.8778\n",
      "[Time 2.2] Poisson Loss: 0.9053\n",
      "[Time 2.3] Poisson Loss: 0.7421\n",
      "[Time 2.4] Poisson Loss: 0.7104\n",
      "[Time 2.5] Poisson Loss: 0.6942\n",
      "[Time 2.6] Poisson Loss: 0.7112\n",
      "[Time 2.7] Poisson Loss: 0.7482\n",
      "[Time 2.8] Poisson Loss: 0.8174\n",
      "[Time 2.9] Poisson Loss: 0.6477\n",
      "[Time 3.0] Poisson Loss: 0.7243\n",
      "[Time 3.1] Poisson Loss: 0.7550\n",
      "[Time 3.2] Poisson Loss: 0.8271\n",
      "[Time 3.3] Poisson Loss: 0.7733\n",
      "[Time 3.4] Poisson Loss: 0.6636\n",
      "[Time 3.5] Poisson Loss: 0.6025\n",
      "[Time 3.6] Poisson Loss: 0.6508\n",
      "[Time 3.7] Poisson Loss: 0.7375\n",
      "[Time 3.8] Poisson Loss: 0.6203\n",
      "[Time 3.9] Poisson Loss: 0.6548\n",
      "[Time 4.0] Poisson Loss: 0.4462\n",
      "[Time 4.1] Poisson Loss: 0.5843\n",
      "[Time 4.2] Poisson Loss: 0.7427\n",
      "[Time 4.3] Poisson Loss: 0.7173\n",
      "[Time 4.4] Poisson Loss: 0.7487\n",
      "[Time 4.5] Poisson Loss: 0.6097\n",
      "[Time 4.6] Poisson Loss: 0.7591\n",
      "[Time 4.7] Poisson Loss: 0.5580\n",
      "[Time 4.8] Poisson Loss: 0.5488\n",
      "[Time 4.9] Poisson Loss: 0.7291\n",
      "[Time 5.0] Poisson Loss: 0.4846\n",
      "[Time 5.1] Poisson Loss: 0.5603\n",
      "[Time 5.2] Poisson Loss: 0.6399\n",
      "[Time 5.3] Poisson Loss: 0.5693\n",
      "[Time 5.4] Poisson Loss: 0.5196\n",
      "[Time 5.5] Poisson Loss: 0.6271\n",
      "[Time 5.6] Poisson Loss: 0.6172\n",
      "[Time 5.7] Poisson Loss: 0.5248\n",
      "[Time 5.8] Poisson Loss: 0.4748\n",
      "[Time 5.9] Poisson Loss: 0.5033\n",
      "[Time 6.0] Poisson Loss: 0.5052\n",
      "[Time 6.1] Poisson Loss: 0.4689\n",
      "[Time 6.2] Poisson Loss: 0.5229\n",
      "[Time 6.3] Poisson Loss: 0.5279\n",
      "[Time 6.4] Poisson Loss: 0.4083\n",
      "[Time 6.5] Poisson Loss: 0.3811\n",
      "[Time 6.6] Poisson Loss: 0.4956\n",
      "[Time 6.7] Poisson Loss: 0.5453\n",
      "[Time 6.8] Poisson Loss: 0.4466\n",
      "[Time 6.9] Poisson Loss: 0.5500\n",
      "[Time 7.0] Poisson Loss: 0.3484\n",
      "[Time 7.1] Poisson Loss: 0.5160\n",
      "[Time 7.2] Poisson Loss: 0.3981\n",
      "[Time 7.3] Poisson Loss: 0.6533\n",
      "[Time 7.4] Poisson Loss: 0.5249\n",
      "[Time 7.5] Poisson Loss: 0.3874\n",
      "[Time 7.6] Poisson Loss: 0.4475\n",
      "[Time 7.7] Poisson Loss: 0.3465\n",
      "[Time 7.8] Poisson Loss: 0.5050\n",
      "[Time 7.9] Poisson Loss: 0.5596\n",
      "[Time 8.0] Poisson Loss: 0.4194\n",
      "[Time 8.1] Poisson Loss: 0.3462\n",
      "[Time 8.2] Poisson Loss: 0.6435\n",
      "[Time 8.3] Poisson Loss: 0.3691\n",
      "[Time 8.4] Poisson Loss: 0.3494\n",
      "[Time 8.5] Poisson Loss: 0.8563\n",
      "[Time 8.6] Poisson Loss: 0.6282\n",
      "[Time 8.7] Poisson Loss: 0.5149\n",
      "[Time 8.8] Poisson Loss: 0.5083\n",
      "[Time 8.9] Poisson Loss: 0.4930\n",
      "[Time 9.0] Poisson Loss: 0.4737\n",
      "[Time 9.1] Poisson Loss: 0.4640\n",
      "[Time 9.2] Poisson Loss: 0.3693\n",
      "[Time 9.3] Poisson Loss: 0.3969\n",
      "[Time 9.4] Poisson Loss: 0.4417\n",
      "[Time 9.5] Poisson Loss: 0.5361\n",
      "[Time 9.6] Poisson Loss: 0.6123\n",
      "[Time 9.7] Poisson Loss: 0.4212\n",
      "[Time 9.8] Poisson Loss: 0.4328\n",
      "[Time 9.9] Poisson Loss: 0.3469\n",
      "[Time 10.0] Poisson Loss: 0.4424\n",
      "[Time 10.1] Poisson Loss: 0.4070\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 020] Loss: 0.3493\n",
      "[Epoch 040] Loss: 0.3211\n",
      "[Epoch 060] Loss: 0.2828\n",
      "[Epoch 080] Loss: 0.2588\n",
      "[Epoch 100] Loss: 0.2395\n",
      "[Epoch 120] Loss: 0.2310\n",
      "[Epoch 140] Loss: 0.2160\n",
      "[Epoch 160] Loss: 0.2094\n",
      "[Epoch 180] Loss: 0.2047\n",
      "[Epoch 200] Loss: 0.1936\n",
      "=== Final 200 Epoch Training Complete ===\n",
      "Test Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/', name='citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "#############################################\n",
    "# 2) (선택) 원하는 방식으로 train_mask 설정\n",
    "#    - 예: 테스트셋 제외 후 1000개 노드 골라서 train_mask 구성 등\n",
    "#    - 여기서는 간단히 data.train_mask 사용\n",
    "#############################################\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# 3. test_mask가 False인 노드들(= non-test 노드)만 골라낸 인덱스\n",
    "non_test_indices = torch.where(~test_mask)[0]  # ~test_mask: test_mask가 False인 노드\n",
    "\n",
    "# 4. non_test_indices 중에서 랜덤하게 1000개 선택\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "# 5. 새로운 train_mask 생성\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "\n",
    "# 6. data.train_mask에 할당\n",
    "train_mask = new_train_mask\n",
    "\n",
    "# 이제 data.train_mask에는 test set과 겹치지 않는 1000개의 노드만 True로 설정됨\n",
    "print(\"Train set size:\", train_mask.sum().item())\n",
    "print(\"Overlap with test set:\", (train_mask & data.test_mask).sum().item())  # 0이어야 정상\n",
    "\n",
    "#############################################\n",
    "# 3) GCN 모델 정의\n",
    "#############################################\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        # 특정 노드만 반환 (Poisson 시뮬레이션 시 사용)\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 4) 모델 및 옵티마이저 정의\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "#############################################\n",
    "# 5) (시나리오 B) 먼저 Poisson 시뮬레이션 기반 학습\n",
    "#############################################\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 10  # 총 시뮬레이션 시간\n",
    "time_step = 0.1\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        # Poisson 이벤트 발생 & train_mask\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            # 다음 이벤트 시간\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    # 학습\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 6) 이제 200 epoch 학습(풀-배치 방식)\n",
    "#############################################\n",
    "model.train()\n",
    "for epoch in range(1, 201):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))  # 전체 노드에 대한 forward\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=== Final 200 Epoch Training Complete ===\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))  # 모든 노드에 대한 예측\n",
    "        pred = out.argmax(dim=1)  # 가장 높은 확률을 가진 클래스를 예측\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  # 정확히 맞춘 개수\n",
    "        acc = correct / data.test_mask.sum().item()  # 정확도 계산\n",
    "    return acc\n",
    "\n",
    "# 🔹 테스트 실행\n",
    "test_acc = test()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ad618-030c-4538-93a8-32fd6bacfa9c",
   "metadata": {},
   "source": [
    "### 이거는 그냥 바닐라 GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac910bc3-9984-4051-a6c1-f65f90eaeed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 1.7826 | Train Acc: 0.5711 | Val Acc: 0.5800 | Test Acc: 0.5270\n",
      "Epoch 010 | Loss: 0.7221 | Train Acc: 0.8439 | Val Acc: 0.8580 | Test Acc: 0.7770\n",
      "Epoch 020 | Loss: 0.4688 | Train Acc: 0.8783 | Val Acc: 0.8840 | Test Acc: 0.7760\n",
      "Epoch 030 | Loss: 0.3683 | Train Acc: 0.9094 | Val Acc: 0.9080 | Test Acc: 0.7550\n",
      "Epoch 040 | Loss: 0.3112 | Train Acc: 0.9300 | Val Acc: 0.9080 | Test Acc: 0.7530\n",
      "Epoch 050 | Loss: 0.2975 | Train Acc: 0.9344 | Val Acc: 0.9120 | Test Acc: 0.7520\n",
      "Epoch 060 | Loss: 0.2806 | Train Acc: 0.9400 | Val Acc: 0.9080 | Test Acc: 0.7550\n",
      "Epoch 070 | Loss: 0.2648 | Train Acc: 0.9394 | Val Acc: 0.9100 | Test Acc: 0.7510\n",
      "Epoch 080 | Loss: 0.2440 | Train Acc: 0.9544 | Val Acc: 0.9260 | Test Acc: 0.7440\n",
      "Epoch 090 | Loss: 0.2425 | Train Acc: 0.9483 | Val Acc: 0.9120 | Test Acc: 0.7530\n",
      "Epoch 100 | Loss: 0.2348 | Train Acc: 0.9483 | Val Acc: 0.9220 | Test Acc: 0.7460\n",
      "Epoch 110 | Loss: 0.2253 | Train Acc: 0.9506 | Val Acc: 0.9180 | Test Acc: 0.7490\n",
      "Epoch 120 | Loss: 0.2084 | Train Acc: 0.9572 | Val Acc: 0.9280 | Test Acc: 0.7500\n",
      "Epoch 130 | Loss: 0.2035 | Train Acc: 0.9600 | Val Acc: 0.9300 | Test Acc: 0.7440\n",
      "Epoch 140 | Loss: 0.2055 | Train Acc: 0.9561 | Val Acc: 0.9340 | Test Acc: 0.7500\n",
      "Epoch 150 | Loss: 0.2012 | Train Acc: 0.9583 | Val Acc: 0.9220 | Test Acc: 0.7360\n",
      "Epoch 160 | Loss: 0.2055 | Train Acc: 0.9589 | Val Acc: 0.9180 | Test Acc: 0.7480\n",
      "Epoch 170 | Loss: 0.2017 | Train Acc: 0.9589 | Val Acc: 0.9180 | Test Acc: 0.7390\n",
      "Epoch 180 | Loss: 0.1934 | Train Acc: 0.9589 | Val Acc: 0.9220 | Test Acc: 0.7440\n",
      "Epoch 190 | Loss: 0.1912 | Train Acc: 0.9644 | Val Acc: 0.9260 | Test Acc: 0.7390\n",
      "Best Test Accuracy: 0.7890 (at Epoch 13)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1. 데이터셋 로드 (Citeseer)\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# 2. Train Mask 생성 (test_mask와 겹치지 않는 노드 1800개 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]  # test_mask가 False인 노드들\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]  # 1800개 샘플링\n",
    "\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "train_mask = new_train_mask\n",
    "\n",
    "# 3. GCN 모델 정의\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 4. 모델, 옵티마이저, 손실 함수 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCN(dataset.num_features, 16, dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# 5. 학습 함수 정의\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 선택된 train_mask만 사용\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 6. 테스트 함수 정의\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)  # 가장 높은 확률을 가진 클래스 선택\n",
    "\n",
    "    accs = []\n",
    "    for mask in [train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = (pred[mask] == data.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs  # [train_acc, val_acc, test_acc]\n",
    "\n",
    "# 7. 학습 진행\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "\n",
    "    # 🔹 Test Accuracy가 가장 높은 순간을 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch  # 언제 최고였는지 저장\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c9b74-cb34-4672-be18-4fe838ca74f2",
   "metadata": {},
   "source": [
    "### 바닐라gcn 4-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d3840e4c-0329-43f6-bc2e-db0d81446d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] Loss: 1.8004 | Test Acc: 0.3750\n",
      "[Epoch 010] Loss: 1.1123 | Test Acc: 0.7600\n",
      "[Epoch 020] Loss: 0.7755 | Test Acc: 0.7800\n",
      "[Epoch 030] Loss: 0.5919 | Test Acc: 0.7670\n",
      "[Epoch 040] Loss: 0.4823 | Test Acc: 0.7660\n",
      "[Epoch 050] Loss: 0.4299 | Test Acc: 0.7620\n",
      "[Epoch 060] Loss: 0.3899 | Test Acc: 0.7530\n",
      "[Epoch 070] Loss: 0.3583 | Test Acc: 0.7600\n",
      "[Epoch 080] Loss: 0.3632 | Test Acc: 0.7440\n",
      "[Epoch 090] Loss: 0.3467 | Test Acc: 0.7400\n",
      "[Epoch 100] Loss: 0.3461 | Test Acc: 0.7500\n",
      "[Epoch 110] Loss: 0.3265 | Test Acc: 0.7460\n",
      "[Epoch 120] Loss: 0.3368 | Test Acc: 0.7480\n",
      "[Epoch 130] Loss: 0.3166 | Test Acc: 0.7460\n",
      "[Epoch 140] Loss: 0.2938 | Test Acc: 0.7440\n",
      "[Epoch 150] Loss: 0.2898 | Test Acc: 0.7380\n",
      "[Epoch 160] Loss: 0.2917 | Test Acc: 0.7430\n",
      "[Epoch 170] Loss: 0.2772 | Test Acc: 0.7300\n",
      "[Epoch 180] Loss: 0.2791 | Test Acc: 0.7490\n",
      "[Epoch 190] Loss: 0.2780 | Test Acc: 0.7410\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.7890 (at Epoch 13)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 1. Citeseer 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ 2. Train Mask 생성 (test_mask와 겹치지 않는 노드 1800개 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "train_mask = new_train_mask  # 새 train_mask 적용\n",
    "\n",
    "# ✅ 3. 4-layer Vanilla GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim1, hidden_dim2, hidden_dim3, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, hidden_dim3)\n",
    "        self.conv4 = GCNConv(hidden_dim3, out_features)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.conv1.lin.weight)\n",
    "        init.xavier_uniform_(self.conv2.lin.weight)\n",
    "        init.xavier_uniform_(self.conv3.lin.weight)\n",
    "        init.xavier_uniform_(self.conv4.lin.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # ✅ 첫 번째 GCN 레이어\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # ✅ 두 번째 GCN 레이어\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # ✅ 세 번째 GCN 레이어\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # ✅ 네 번째 GCN 레이어 (출력)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ✅ 4. 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim1=64, \n",
    "    hidden_dim2=32, \n",
    "    hidden_dim3=16, \n",
    "    out_features=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 5. 학습 루프\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 Best Test Accuracy 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbcb1c-e0ab-47fe-a530-5a84cc212f94",
   "metadata": {},
   "source": [
    "### 이거 4-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d8ac1c-6c00-4893-9c25-e53b4d098e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 0.2] Poisson Loss: 1.7969\n",
      "[Time 0.3] Poisson Loss: 1.7786\n",
      "[Time 0.4] Poisson Loss: 1.7631\n",
      "[Time 0.5] Poisson Loss: 1.7004\n",
      "[Time 0.6] Poisson Loss: 1.6389\n",
      "[Time 0.7] Poisson Loss: 1.5969\n",
      "[Time 0.8] Poisson Loss: 1.5479\n",
      "[Time 0.9] Poisson Loss: 1.4088\n",
      "[Time 1.0] Poisson Loss: 1.3526\n",
      "[Time 1.1] Poisson Loss: 1.3874\n",
      "[Time 1.2] Poisson Loss: 1.3544\n",
      "[Time 1.3] Poisson Loss: 1.2161\n",
      "[Time 1.4] Poisson Loss: 1.2222\n",
      "[Time 1.5] Poisson Loss: 1.2324\n",
      "[Time 1.6] Poisson Loss: 1.1598\n",
      "[Time 1.7] Poisson Loss: 1.0959\n",
      "[Time 1.8] Poisson Loss: 1.0752\n",
      "[Time 1.9] Poisson Loss: 0.9887\n",
      "[Time 2.0] Poisson Loss: 1.0838\n",
      "[Time 2.1] Poisson Loss: 0.9889\n",
      "[Time 2.2] Poisson Loss: 0.9808\n",
      "[Time 2.3] Poisson Loss: 0.9716\n",
      "[Time 2.4] Poisson Loss: 0.9800\n",
      "[Time 2.5] Poisson Loss: 1.0090\n",
      "[Time 2.6] Poisson Loss: 0.9331\n",
      "[Time 2.7] Poisson Loss: 0.9495\n",
      "[Time 2.8] Poisson Loss: 0.9044\n",
      "[Time 2.9] Poisson Loss: 0.9706\n",
      "[Time 3.0] Poisson Loss: 0.8588\n",
      "[Time 3.1] Poisson Loss: 0.8651\n",
      "[Time 3.2] Poisson Loss: 0.8383\n",
      "[Time 3.3] Poisson Loss: 0.6295\n",
      "[Time 3.4] Poisson Loss: 0.8125\n",
      "[Time 3.5] Poisson Loss: 0.8985\n",
      "[Time 3.6] Poisson Loss: 0.7718\n",
      "[Time 3.7] Poisson Loss: 0.7755\n",
      "[Time 3.8] Poisson Loss: 0.7066\n",
      "[Time 3.9] Poisson Loss: 0.6594\n",
      "[Time 4.0] Poisson Loss: 0.5706\n",
      "[Time 4.1] Poisson Loss: 0.8992\n",
      "[Time 4.2] Poisson Loss: 0.7389\n",
      "[Time 4.3] Poisson Loss: 0.6764\n",
      "[Time 4.4] Poisson Loss: 0.6762\n",
      "[Time 4.5] Poisson Loss: 0.5959\n",
      "[Time 4.6] Poisson Loss: 0.7018\n",
      "[Time 4.7] Poisson Loss: 0.8273\n",
      "[Time 4.8] Poisson Loss: 0.5050\n",
      "[Time 4.9] Poisson Loss: 0.8257\n",
      "[Time 5.0] Poisson Loss: 0.5530\n",
      "[Time 5.1] Poisson Loss: 0.4809\n",
      "[Time 5.2] Poisson Loss: 0.6531\n",
      "[Time 5.3] Poisson Loss: 0.5148\n",
      "[Time 5.4] Poisson Loss: 0.3995\n",
      "[Time 5.5] Poisson Loss: 0.6828\n",
      "[Time 5.6] Poisson Loss: 0.5350\n",
      "[Time 5.7] Poisson Loss: 0.5156\n",
      "[Time 5.8] Poisson Loss: 0.8087\n",
      "[Time 5.9] Poisson Loss: 0.7557\n",
      "[Time 6.0] Poisson Loss: 0.5461\n",
      "[Time 6.1] Poisson Loss: 0.6562\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 000] Loss: 0.5846 | Test Acc: 0.7620\n",
      "[Epoch 010] Loss: 0.5127 | Test Acc: 0.7570\n",
      "[Epoch 020] Loss: 0.4298 | Test Acc: 0.7600\n",
      "[Epoch 030] Loss: 0.3713 | Test Acc: 0.7540\n",
      "[Epoch 040] Loss: 0.3495 | Test Acc: 0.7490\n",
      "[Epoch 050] Loss: 0.3061 | Test Acc: 0.7510\n",
      "[Epoch 060] Loss: 0.2834 | Test Acc: 0.7470\n",
      "[Epoch 070] Loss: 0.2668 | Test Acc: 0.7500\n",
      "[Epoch 080] Loss: 0.2435 | Test Acc: 0.7370\n",
      "[Epoch 090] Loss: 0.2374 | Test Acc: 0.7440\n",
      "[Epoch 100] Loss: 0.2314 | Test Acc: 0.7410\n",
      "[Epoch 110] Loss: 0.2201 | Test Acc: 0.7420\n",
      "[Epoch 120] Loss: 0.2101 | Test Acc: 0.7270\n",
      "[Epoch 130] Loss: 0.1961 | Test Acc: 0.7430\n",
      "[Epoch 140] Loss: 0.2067 | Test Acc: 0.7220\n",
      "[Epoch 150] Loss: 0.1857 | Test Acc: 0.7390\n",
      "[Epoch 160] Loss: 0.1820 | Test Acc: 0.7330\n",
      "[Epoch 170] Loss: 0.1776 | Test Acc: 0.7310\n",
      "[Epoch 180] Loss: 0.1785 | Test Acc: 0.7310\n",
      "[Epoch 190] Loss: 0.1685 | Test Acc: 0.7260\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.7620 (at Epoch 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 1. Citeseer 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ 2. Train Mask 생성 (test_mask와 겹치지 않는 1800개 노드 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "train_mask = new_train_mask  # 새 train_mask 적용\n",
    "\n",
    "# ✅ 3. 4-layer GCN 모델 (Xavier 제거)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim1, hidden_dim2, hidden_dim3, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, hidden_dim3)\n",
    "        self.conv4 = GCNConv(hidden_dim3, out_features)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "# ✅ 4. 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim1=64, \n",
    "    hidden_dim2=32, \n",
    "    hidden_dim3=16, \n",
    "    out_features=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 5. Poisson 시뮬레이션 기반 학습\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 6  # 총 시뮬레이션 시간\n",
    "time_step = 0.1\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "# ✅ 6. 이제 200 epoch 학습 (풀-배치 방식, Best Test Accuracy 저장)\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 Test Accuracy가 가장 높은 순간 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch  # 언제 최고였는지 저장\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7eaefa-5f80-475b-9a67-3d4f1af2666b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8865978-2b28-4464-ba7e-740e243d064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] Loss: 2.1425 | Test Acc: 0.3440\n",
      "[Epoch 010] Loss: 0.5542 | Test Acc: 0.7180\n",
      "[Epoch 020] Loss: 0.3326 | Test Acc: 0.7640\n",
      "[Epoch 030] Loss: 0.2069 | Test Acc: 0.7600\n",
      "[Epoch 040] Loss: 0.1510 | Test Acc: 0.7660\n",
      "[Epoch 050] Loss: 0.1123 | Test Acc: 0.7590\n",
      "[Epoch 060] Loss: 0.0867 | Test Acc: 0.7560\n",
      "[Epoch 070] Loss: 0.0675 | Test Acc: 0.7540\n",
      "[Epoch 080] Loss: 0.0588 | Test Acc: 0.7520\n",
      "[Epoch 090] Loss: 0.0487 | Test Acc: 0.7510\n",
      "[Epoch 100] Loss: 0.0437 | Test Acc: 0.7470\n",
      "[Epoch 110] Loss: 0.0386 | Test Acc: 0.7470\n",
      "[Epoch 120] Loss: 0.0338 | Test Acc: 0.7470\n",
      "[Epoch 130] Loss: 0.0324 | Test Acc: 0.7440\n",
      "[Epoch 140] Loss: 0.0307 | Test Acc: 0.7450\n",
      "[Epoch 150] Loss: 0.0256 | Test Acc: 0.7470\n",
      "[Epoch 160] Loss: 0.0252 | Test Acc: 0.7440\n",
      "[Epoch 170] Loss: 0.0241 | Test Acc: 0.7400\n",
      "[Epoch 180] Loss: 0.0213 | Test Acc: 0.7390\n",
      "[Epoch 190] Loss: 0.0196 | Test Acc: 0.7400\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.7700 (at Epoch 17)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn import BatchNorm1d, Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 1. Citeseer 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ 2. Train Mask 생성 (test_mask와 겹치지 않는 노드 1800개 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "train_mask = new_train_mask  # 새 train_mask 적용\n",
    "\n",
    "# ✅ 3. 3-layer GCN + Projection Layer 추가\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim1, hidden_dim2, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, out_features)\n",
    "\n",
    "        self.bn1 = BatchNorm1d(hidden_dim1)\n",
    "        self.bn2 = BatchNorm1d(hidden_dim2)\n",
    "\n",
    "        # ✅ Projection Layer 추가 (입력 차원과 맞춤)\n",
    "        self.projection = Linear(in_features, out_features)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.conv1.lin.weight)\n",
    "        init.xavier_uniform_(self.conv2.lin.weight)\n",
    "        init.xavier_uniform_(self.conv3.lin.weight)\n",
    "        init.xavier_uniform_(self.projection.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # ✅ Residual Connection을 위한 원본 특징 저장\n",
    "        x_res = self.projection(x)  # 입력 차원과 출력 차원을 맞춰줌\n",
    "\n",
    "        # ✅ 첫 번째 GCN 레이어\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # ✅ 두 번째 GCN 레이어\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # ✅ 세 번째 GCN 레이어 (출력)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # ✅ Residual Connection 추가\n",
    "        x += x_res  # 원래 입력 특징을 더해줌 (차원 맞춤)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ✅ 4. 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim1=64, \n",
    "    hidden_dim2=32, \n",
    "    out_features=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 5. 학습 루프\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 Best Test Accuracy 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3596139-4438-4084-aabd-6481a6211a6c",
   "metadata": {},
   "source": [
    "### 이번에는 Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590b2dc5-78dc-4b9f-9ed7-4c057db515a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1200\n",
      "Overlap with test set: 0\n",
      "[Time 0.2] Poisson Loss: 1.9569\n",
      "[Time 0.3] Poisson Loss: 1.8972\n",
      "[Time 0.4] Poisson Loss: 1.8401\n",
      "[Time 0.5] Poisson Loss: 1.7912\n",
      "[Time 0.6] Poisson Loss: 1.7283\n",
      "[Time 0.7] Poisson Loss: 1.6761\n",
      "[Time 0.8] Poisson Loss: 1.4945\n",
      "[Time 0.9] Poisson Loss: 1.4464\n",
      "[Time 1.0] Poisson Loss: 1.4060\n",
      "[Time 1.1] Poisson Loss: 1.4227\n",
      "[Time 1.2] Poisson Loss: 1.2878\n",
      "[Time 1.3] Poisson Loss: 1.2073\n",
      "[Time 1.4] Poisson Loss: 1.2002\n",
      "[Time 1.5] Poisson Loss: 1.0866\n",
      "[Time 1.6] Poisson Loss: 1.0475\n",
      "[Time 1.7] Poisson Loss: 1.1163\n",
      "[Time 1.8] Poisson Loss: 0.9510\n",
      "[Time 1.9] Poisson Loss: 0.9116\n",
      "[Time 2.0] Poisson Loss: 0.8285\n",
      "[Time 2.1] Poisson Loss: 0.8101\n",
      "[Time 2.2] Poisson Loss: 0.8257\n",
      "[Time 2.3] Poisson Loss: 0.7444\n",
      "[Time 2.4] Poisson Loss: 0.7504\n",
      "[Time 2.5] Poisson Loss: 0.6202\n",
      "[Time 2.6] Poisson Loss: 0.7280\n",
      "[Time 2.7] Poisson Loss: 0.7471\n",
      "[Time 2.8] Poisson Loss: 0.7894\n",
      "[Time 2.9] Poisson Loss: 0.6880\n",
      "[Time 3.0] Poisson Loss: 0.6304\n",
      "[Time 3.1] Poisson Loss: 0.5061\n",
      "[Time 3.2] Poisson Loss: 0.6881\n",
      "[Time 3.3] Poisson Loss: 0.5924\n",
      "[Time 3.4] Poisson Loss: 0.5246\n",
      "[Time 3.5] Poisson Loss: 0.5938\n",
      "[Time 3.6] Poisson Loss: 0.4776\n",
      "[Time 3.7] Poisson Loss: 0.4281\n",
      "[Time 3.8] Poisson Loss: 0.5583\n",
      "[Time 3.9] Poisson Loss: 0.5977\n",
      "[Time 4.0] Poisson Loss: 0.3604\n",
      "[Time 4.1] Poisson Loss: 0.3955\n",
      "[Time 4.2] Poisson Loss: 0.3980\n",
      "[Time 4.3] Poisson Loss: 0.4091\n",
      "[Time 4.4] Poisson Loss: 0.4505\n",
      "[Time 4.5] Poisson Loss: 0.4584\n",
      "[Time 4.6] Poisson Loss: 0.3631\n",
      "[Time 4.7] Poisson Loss: 0.2375\n",
      "[Time 4.8] Poisson Loss: 0.4485\n",
      "[Time 4.9] Poisson Loss: 0.3852\n",
      "[Time 5.0] Poisson Loss: 0.4704\n",
      "[Time 5.1] Poisson Loss: 0.3023\n",
      "[Time 5.2] Poisson Loss: 0.3166\n",
      "[Time 5.3] Poisson Loss: 0.2563\n",
      "[Time 5.4] Poisson Loss: 0.4230\n",
      "[Time 5.5] Poisson Loss: 0.3271\n",
      "[Time 5.6] Poisson Loss: 0.4303\n",
      "[Time 5.7] Poisson Loss: 0.3971\n",
      "[Time 5.8] Poisson Loss: 0.4014\n",
      "[Time 5.9] Poisson Loss: 0.2432\n",
      "[Time 6.0] Poisson Loss: 0.4055\n",
      "[Time 6.1] Poisson Loss: 0.4598\n",
      "[Time 6.2] Poisson Loss: 0.2444\n",
      "[Time 6.3] Poisson Loss: 0.4000\n",
      "[Time 6.4] Poisson Loss: 0.2289\n",
      "[Time 6.5] Poisson Loss: 0.3934\n",
      "[Time 6.6] Poisson Loss: 0.3045\n",
      "[Time 6.7] Poisson Loss: 0.3116\n",
      "[Time 6.8] Poisson Loss: 0.3867\n",
      "[Time 6.9] Poisson Loss: 0.2247\n",
      "[Time 7.0] Poisson Loss: 0.2928\n",
      "[Time 7.1] Poisson Loss: 0.2746\n",
      "[Time 7.2] Poisson Loss: 0.3935\n",
      "[Time 7.3] Poisson Loss: 0.3786\n",
      "[Time 7.4] Poisson Loss: 0.2688\n",
      "[Time 7.5] Poisson Loss: 0.2052\n",
      "[Time 7.6] Poisson Loss: 0.1499\n",
      "[Time 7.7] Poisson Loss: 0.2821\n",
      "[Time 7.8] Poisson Loss: 0.4195\n",
      "[Time 7.9] Poisson Loss: 0.2551\n",
      "[Time 8.0] Poisson Loss: 0.2978\n",
      "[Time 8.1] Poisson Loss: 0.2182\n",
      "[Time 8.2] Poisson Loss: 0.2216\n",
      "[Time 8.3] Poisson Loss: 0.3357\n",
      "[Time 8.4] Poisson Loss: 0.2236\n",
      "[Time 8.5] Poisson Loss: 0.1949\n",
      "[Time 8.6] Poisson Loss: 0.3537\n",
      "[Time 8.7] Poisson Loss: 0.2542\n",
      "[Time 8.8] Poisson Loss: 0.2656\n",
      "[Time 8.9] Poisson Loss: 0.2601\n",
      "[Time 9.0] Poisson Loss: 0.3620\n",
      "[Time 9.1] Poisson Loss: 0.3988\n",
      "[Time 9.2] Poisson Loss: 0.2102\n",
      "[Time 9.3] Poisson Loss: 0.1943\n",
      "[Time 9.4] Poisson Loss: 0.1685\n",
      "[Time 9.5] Poisson Loss: 0.1805\n",
      "[Time 9.6] Poisson Loss: 0.3036\n",
      "[Time 9.7] Poisson Loss: 0.1945\n",
      "[Time 9.8] Poisson Loss: 0.4379\n",
      "[Time 9.9] Poisson Loss: 0.2804\n",
      "[Time 10.0] Poisson Loss: 0.3005\n",
      "[Time 10.1] Poisson Loss: 0.2352\n",
      "[Time 10.2] Poisson Loss: 0.3175\n",
      "[Time 10.3] Poisson Loss: 0.3584\n",
      "[Time 10.4] Poisson Loss: 0.2781\n",
      "[Time 10.5] Poisson Loss: 0.3157\n",
      "[Time 10.6] Poisson Loss: 0.2957\n",
      "[Time 10.7] Poisson Loss: 0.2186\n",
      "[Time 10.8] Poisson Loss: 0.2127\n",
      "[Time 10.9] Poisson Loss: 0.2456\n",
      "[Time 11.0] Poisson Loss: 0.3416\n",
      "[Time 11.1] Poisson Loss: 0.3515\n",
      "[Time 11.2] Poisson Loss: 0.2339\n",
      "[Time 11.3] Poisson Loss: 0.1122\n",
      "[Time 11.4] Poisson Loss: 0.2224\n",
      "[Time 11.5] Poisson Loss: 0.3696\n",
      "[Time 11.6] Poisson Loss: 0.2764\n",
      "[Time 11.7] Poisson Loss: 0.2351\n",
      "[Time 11.8] Poisson Loss: 0.3299\n",
      "[Time 11.9] Poisson Loss: 0.2452\n",
      "[Time 12.0] Poisson Loss: 0.1864\n",
      "[Time 12.1] Poisson Loss: 0.2039\n",
      "[Time 12.2] Poisson Loss: 0.2651\n",
      "[Time 12.3] Poisson Loss: 0.3178\n",
      "[Time 12.4] Poisson Loss: 0.4290\n",
      "[Time 12.5] Poisson Loss: 0.2181\n",
      "[Time 12.6] Poisson Loss: 0.2783\n",
      "[Time 12.7] Poisson Loss: 0.1816\n",
      "[Time 12.8] Poisson Loss: 0.1649\n",
      "[Time 12.9] Poisson Loss: 0.1001\n",
      "[Time 13.0] Poisson Loss: 0.2705\n",
      "[Time 13.1] Poisson Loss: 0.1749\n",
      "[Time 13.2] Poisson Loss: 0.1889\n",
      "[Time 13.3] Poisson Loss: 0.3138\n",
      "[Time 13.4] Poisson Loss: 0.2751\n",
      "[Time 13.5] Poisson Loss: 0.3071\n",
      "[Time 13.6] Poisson Loss: 0.1884\n",
      "[Time 13.7] Poisson Loss: 0.2005\n",
      "[Time 13.8] Poisson Loss: 0.1082\n",
      "[Time 13.9] Poisson Loss: 0.1295\n",
      "[Time 14.0] Poisson Loss: 0.1704\n",
      "[Time 14.1] Poisson Loss: 0.1720\n",
      "[Time 14.2] Poisson Loss: 0.2563\n",
      "[Time 14.3] Poisson Loss: 0.3191\n",
      "[Time 14.4] Poisson Loss: 0.2136\n",
      "[Time 14.5] Poisson Loss: 0.1241\n",
      "[Time 14.6] Poisson Loss: 0.1778\n",
      "[Time 14.7] Poisson Loss: 0.1573\n",
      "[Time 14.8] Poisson Loss: 0.1516\n",
      "[Time 14.9] Poisson Loss: 0.2459\n",
      "[Time 15.0] Poisson Loss: 0.3225\n",
      "[Time 15.1] Poisson Loss: 0.1598\n",
      "[Time 15.2] Poisson Loss: 0.2237\n",
      "[Time 15.3] Poisson Loss: 0.2077\n",
      "[Time 15.4] Poisson Loss: 0.3304\n",
      "[Time 15.5] Poisson Loss: 0.0884\n",
      "[Time 15.6] Poisson Loss: 0.1449\n",
      "[Time 15.7] Poisson Loss: 0.2278\n",
      "[Time 15.8] Poisson Loss: 0.0855\n",
      "[Time 15.9] Poisson Loss: 0.2262\n",
      "[Time 16.0] Poisson Loss: 0.3239\n",
      "[Time 16.1] Poisson Loss: 0.2835\n",
      "[Time 16.2] Poisson Loss: 0.1590\n",
      "[Time 16.3] Poisson Loss: 0.1545\n",
      "[Time 16.4] Poisson Loss: 0.2422\n",
      "[Time 16.5] Poisson Loss: 0.1462\n",
      "[Time 16.6] Poisson Loss: 0.1962\n",
      "[Time 16.7] Poisson Loss: 0.2869\n",
      "[Time 16.8] Poisson Loss: 0.2341\n",
      "[Time 16.9] Poisson Loss: 0.1299\n",
      "[Time 17.0] Poisson Loss: 0.1281\n",
      "[Time 17.1] Poisson Loss: 0.1395\n",
      "[Time 17.2] Poisson Loss: 0.2737\n",
      "[Time 17.3] Poisson Loss: 0.1550\n",
      "[Time 17.4] Poisson Loss: 0.2790\n",
      "[Time 17.5] Poisson Loss: 0.1371\n",
      "[Time 17.6] Poisson Loss: 0.1042\n",
      "[Time 17.7] Poisson Loss: 0.1679\n",
      "[Time 17.8] Poisson Loss: 0.2326\n",
      "[Time 17.9] Poisson Loss: 0.3445\n",
      "[Time 18.0] Poisson Loss: 0.1935\n",
      "[Time 18.1] Poisson Loss: 0.2797\n",
      "[Time 18.2] Poisson Loss: 0.2645\n",
      "[Time 18.3] Poisson Loss: 0.3179\n",
      "[Time 18.4] Poisson Loss: 0.0545\n",
      "[Time 18.5] Poisson Loss: 0.1298\n",
      "[Time 18.6] Poisson Loss: 0.2438\n",
      "[Time 18.7] Poisson Loss: 0.2352\n",
      "[Time 18.8] Poisson Loss: 0.1363\n",
      "[Time 18.9] Poisson Loss: 0.1800\n",
      "[Time 19.0] Poisson Loss: 0.2590\n",
      "[Time 19.1] Poisson Loss: 0.2380\n",
      "[Time 19.2] Poisson Loss: 0.2022\n",
      "[Time 19.3] Poisson Loss: 0.1853\n",
      "[Time 19.4] Poisson Loss: 0.1920\n",
      "[Time 19.5] Poisson Loss: 0.1401\n",
      "[Time 19.6] Poisson Loss: 0.1770\n",
      "[Time 19.7] Poisson Loss: 0.1782\n",
      "[Time 19.8] Poisson Loss: 0.1879\n",
      "[Time 19.9] Poisson Loss: 0.1427\n",
      "[Time 20.0] Poisson Loss: 0.1558\n",
      "[Time 20.1] Poisson Loss: 0.2029\n",
      "[Time 20.2] Poisson Loss: 0.1912\n",
      "[Time 20.3] Poisson Loss: 0.2260\n",
      "[Time 20.4] Poisson Loss: 0.1886\n",
      "[Time 20.5] Poisson Loss: 0.1802\n",
      "[Time 20.6] Poisson Loss: 0.1624\n",
      "[Time 20.7] Poisson Loss: 0.1469\n",
      "[Time 20.8] Poisson Loss: 0.1218\n",
      "[Time 20.9] Poisson Loss: 0.1452\n",
      "[Time 21.0] Poisson Loss: 0.2204\n",
      "[Time 21.1] Poisson Loss: 0.1369\n",
      "[Time 21.2] Poisson Loss: 0.1442\n",
      "[Time 21.3] Poisson Loss: 0.2163\n",
      "[Time 21.4] Poisson Loss: 0.1608\n",
      "[Time 21.5] Poisson Loss: 0.1807\n",
      "[Time 21.6] Poisson Loss: 0.2919\n",
      "[Time 21.7] Poisson Loss: 0.2697\n",
      "[Time 21.8] Poisson Loss: 0.0946\n",
      "[Time 21.9] Poisson Loss: 0.1362\n",
      "[Time 22.0] Poisson Loss: 0.1651\n",
      "[Time 22.1] Poisson Loss: 0.2534\n",
      "[Time 22.2] Poisson Loss: 0.2086\n",
      "[Time 22.3] Poisson Loss: 0.2719\n",
      "[Time 22.4] Poisson Loss: 0.3333\n",
      "[Time 22.5] Poisson Loss: 0.3012\n",
      "[Time 22.6] Poisson Loss: 0.1361\n",
      "[Time 22.7] Poisson Loss: 0.1188\n",
      "[Time 22.8] Poisson Loss: 0.0864\n",
      "[Time 22.9] Poisson Loss: 0.0941\n",
      "[Time 23.0] Poisson Loss: 0.1850\n",
      "[Time 23.1] Poisson Loss: 0.2791\n",
      "[Time 23.2] Poisson Loss: 0.2169\n",
      "[Time 23.3] Poisson Loss: 0.1424\n",
      "[Time 23.4] Poisson Loss: 0.1806\n",
      "[Time 23.5] Poisson Loss: 0.1759\n",
      "[Time 23.6] Poisson Loss: 0.3706\n",
      "[Time 23.7] Poisson Loss: 0.2380\n",
      "[Time 23.8] Poisson Loss: 0.1131\n",
      "[Time 23.9] Poisson Loss: 0.1771\n",
      "[Time 24.0] Poisson Loss: 0.2342\n",
      "[Time 24.1] Poisson Loss: 0.1780\n",
      "[Time 24.2] Poisson Loss: 0.2283\n",
      "[Time 24.3] Poisson Loss: 0.1137\n",
      "[Time 24.4] Poisson Loss: 0.1266\n",
      "[Time 24.5] Poisson Loss: 0.1098\n",
      "[Time 24.6] Poisson Loss: 0.1889\n",
      "[Time 24.7] Poisson Loss: 0.0831\n",
      "[Time 24.8] Poisson Loss: 0.2013\n",
      "[Time 24.9] Poisson Loss: 0.1428\n",
      "[Time 25.0] Poisson Loss: 0.1198\n",
      "[Time 25.1] Poisson Loss: 0.1465\n",
      "[Time 25.2] Poisson Loss: 0.1459\n",
      "[Time 25.3] Poisson Loss: 0.1137\n",
      "[Time 25.4] Poisson Loss: 0.0966\n",
      "[Time 25.5] Poisson Loss: 0.1821\n",
      "[Time 25.6] Poisson Loss: 0.1467\n",
      "[Time 25.7] Poisson Loss: 0.1319\n",
      "[Time 25.8] Poisson Loss: 0.1236\n",
      "[Time 25.9] Poisson Loss: 0.1108\n",
      "[Time 26.0] Poisson Loss: 0.1747\n",
      "[Time 26.1] Poisson Loss: 0.1717\n",
      "[Time 26.2] Poisson Loss: 0.2614\n",
      "[Time 26.3] Poisson Loss: 0.2528\n",
      "[Time 26.4] Poisson Loss: 0.2417\n",
      "[Time 26.5] Poisson Loss: 0.1361\n",
      "[Time 26.6] Poisson Loss: 0.1786\n",
      "[Time 26.7] Poisson Loss: 0.1273\n",
      "[Time 26.8] Poisson Loss: 0.1866\n",
      "[Time 26.9] Poisson Loss: 0.1845\n",
      "[Time 27.0] Poisson Loss: 0.1920\n",
      "[Time 27.1] Poisson Loss: 0.0948\n",
      "[Time 27.2] Poisson Loss: 0.1239\n",
      "[Time 27.3] Poisson Loss: 0.3092\n",
      "[Time 27.4] Poisson Loss: 0.1813\n",
      "[Time 27.5] Poisson Loss: 0.2020\n",
      "[Time 27.6] Poisson Loss: 0.2422\n",
      "[Time 27.7] Poisson Loss: 0.1220\n",
      "[Time 27.8] Poisson Loss: 0.1688\n",
      "[Time 27.9] Poisson Loss: 0.2526\n",
      "[Time 28.0] Poisson Loss: 0.2565\n",
      "[Time 28.1] Poisson Loss: 0.1263\n",
      "[Time 28.2] Poisson Loss: 0.0441\n",
      "[Time 28.3] Poisson Loss: 0.1057\n",
      "[Time 28.4] Poisson Loss: 0.2178\n",
      "[Time 28.5] Poisson Loss: 0.2252\n",
      "[Time 28.6] Poisson Loss: 0.1942\n",
      "[Time 28.7] Poisson Loss: 0.2022\n",
      "[Time 28.8] Poisson Loss: 0.2470\n",
      "[Time 28.9] Poisson Loss: 0.1081\n",
      "[Time 29.0] Poisson Loss: 0.1362\n",
      "[Time 29.1] Poisson Loss: 0.1095\n",
      "[Time 29.2] Poisson Loss: 0.2722\n",
      "[Time 29.3] Poisson Loss: 0.1786\n",
      "[Time 29.4] Poisson Loss: 0.2634\n",
      "[Time 29.5] Poisson Loss: 0.2045\n",
      "[Time 29.6] Poisson Loss: 0.1569\n",
      "[Time 29.7] Poisson Loss: 0.1447\n",
      "[Time 29.8] Poisson Loss: 0.0970\n",
      "[Time 29.9] Poisson Loss: 0.1627\n",
      "[Time 30.0] Poisson Loss: 0.1991\n",
      "[Time 30.1] Poisson Loss: 0.1685\n",
      "[Time 30.2] Poisson Loss: 0.1483\n",
      "[Time 30.3] Poisson Loss: 0.2666\n",
      "[Time 30.4] Poisson Loss: 0.1991\n",
      "[Time 30.5] Poisson Loss: 0.1544\n",
      "[Time 30.6] Poisson Loss: 0.1550\n",
      "[Time 30.7] Poisson Loss: 0.2948\n",
      "[Time 30.8] Poisson Loss: 0.1179\n",
      "[Time 30.9] Poisson Loss: 0.1656\n",
      "[Time 31.0] Poisson Loss: 0.1633\n",
      "[Time 31.1] Poisson Loss: 0.0869\n",
      "[Time 31.2] Poisson Loss: 0.2002\n",
      "[Time 31.3] Poisson Loss: 0.0961\n",
      "[Time 31.4] Poisson Loss: 0.2976\n",
      "[Time 31.5] Poisson Loss: 0.1336\n",
      "[Time 31.6] Poisson Loss: 0.1609\n",
      "[Time 31.7] Poisson Loss: 0.0772\n",
      "[Time 31.8] Poisson Loss: 0.1179\n",
      "[Time 31.9] Poisson Loss: 0.2054\n",
      "[Time 32.0] Poisson Loss: 0.2167\n",
      "[Time 32.1] Poisson Loss: 0.1292\n",
      "[Time 32.2] Poisson Loss: 0.1313\n",
      "[Time 32.3] Poisson Loss: 0.1494\n",
      "[Time 32.4] Poisson Loss: 0.1546\n",
      "[Time 32.5] Poisson Loss: 0.0876\n",
      "[Time 32.6] Poisson Loss: 0.1637\n",
      "[Time 32.7] Poisson Loss: 0.3668\n",
      "[Time 32.8] Poisson Loss: 0.1282\n",
      "[Time 32.9] Poisson Loss: 0.1428\n",
      "[Time 33.0] Poisson Loss: 0.1606\n",
      "[Time 33.1] Poisson Loss: 0.1055\n",
      "[Time 33.2] Poisson Loss: 0.0824\n",
      "[Time 33.3] Poisson Loss: 0.1493\n",
      "[Time 33.4] Poisson Loss: 0.0891\n",
      "[Time 33.5] Poisson Loss: 0.1929\n",
      "[Time 33.6] Poisson Loss: 0.1492\n",
      "[Time 33.7] Poisson Loss: 0.2097\n",
      "[Time 33.8] Poisson Loss: 0.2702\n",
      "[Time 33.9] Poisson Loss: 0.1772\n",
      "[Time 34.0] Poisson Loss: 0.1368\n",
      "[Time 34.1] Poisson Loss: 0.1038\n",
      "[Time 34.2] Poisson Loss: 0.1924\n",
      "[Time 34.3] Poisson Loss: 0.1401\n",
      "[Time 34.4] Poisson Loss: 0.2413\n",
      "[Time 34.5] Poisson Loss: 0.1897\n",
      "[Time 34.6] Poisson Loss: 0.1101\n",
      "[Time 34.7] Poisson Loss: 0.2033\n",
      "[Time 34.8] Poisson Loss: 0.1023\n",
      "[Time 34.9] Poisson Loss: 0.3418\n",
      "[Time 35.0] Poisson Loss: 0.1290\n",
      "[Time 35.1] Poisson Loss: 0.1099\n",
      "[Time 35.2] Poisson Loss: 0.1241\n",
      "[Time 35.3] Poisson Loss: 0.0828\n",
      "[Time 35.4] Poisson Loss: 0.1702\n",
      "[Time 35.5] Poisson Loss: 0.2827\n",
      "[Time 35.6] Poisson Loss: 0.1522\n",
      "[Time 35.7] Poisson Loss: 0.0622\n",
      "[Time 35.8] Poisson Loss: 0.1180\n",
      "[Time 35.9] Poisson Loss: 0.1198\n",
      "[Time 36.0] Poisson Loss: 0.0809\n",
      "[Time 36.1] Poisson Loss: 0.1935\n",
      "[Time 36.2] Poisson Loss: 0.1351\n",
      "[Time 36.3] Poisson Loss: 0.2015\n",
      "[Time 36.4] Poisson Loss: 0.1701\n",
      "[Time 36.5] Poisson Loss: 0.1562\n",
      "[Time 36.6] Poisson Loss: 0.2061\n",
      "[Time 36.7] Poisson Loss: 0.1356\n",
      "[Time 36.8] Poisson Loss: 0.1138\n",
      "[Time 36.9] Poisson Loss: 0.1799\n",
      "[Time 37.0] Poisson Loss: 0.0744\n",
      "[Time 37.1] Poisson Loss: 0.0833\n",
      "[Time 37.2] Poisson Loss: 0.1909\n",
      "[Time 37.3] Poisson Loss: 0.2004\n",
      "[Time 37.4] Poisson Loss: 0.1500\n",
      "[Time 37.5] Poisson Loss: 0.2355\n",
      "[Time 37.6] Poisson Loss: 0.2342\n",
      "[Time 37.7] Poisson Loss: 0.1836\n",
      "[Time 37.8] Poisson Loss: 0.1258\n",
      "[Time 37.9] Poisson Loss: 0.0674\n",
      "[Time 38.0] Poisson Loss: 0.1417\n",
      "[Time 38.1] Poisson Loss: 0.2517\n",
      "[Time 38.2] Poisson Loss: 0.1519\n",
      "[Time 38.3] Poisson Loss: 0.1654\n",
      "[Time 38.4] Poisson Loss: 0.0868\n",
      "[Time 38.5] Poisson Loss: 0.1329\n",
      "[Time 38.6] Poisson Loss: 0.2515\n",
      "[Time 38.7] Poisson Loss: 0.1185\n",
      "[Time 38.8] Poisson Loss: 0.1462\n",
      "[Time 38.9] Poisson Loss: 0.2247\n",
      "[Time 39.0] Poisson Loss: 0.1734\n",
      "[Time 39.1] Poisson Loss: 0.1030\n",
      "[Time 39.2] Poisson Loss: 0.1138\n",
      "[Time 39.3] Poisson Loss: 0.1855\n",
      "[Time 39.4] Poisson Loss: 0.2175\n",
      "[Time 39.5] Poisson Loss: 0.1860\n",
      "[Time 39.6] Poisson Loss: 0.1394\n",
      "[Time 39.7] Poisson Loss: 0.2064\n",
      "[Time 39.8] Poisson Loss: 0.2039\n",
      "[Time 39.9] Poisson Loss: 0.1880\n",
      "[Time 40.0] Poisson Loss: 0.1087\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 020] Loss: 0.1319\n",
      "[Epoch 040] Loss: 0.1244\n",
      "[Epoch 060] Loss: 0.1030\n",
      "[Epoch 080] Loss: 0.1074\n",
      "[Epoch 100] Loss: 0.1145\n",
      "[Epoch 120] Loss: 0.1174\n",
      "[Epoch 140] Loss: 0.1154\n",
      "[Epoch 160] Loss: 0.1134\n",
      "[Epoch 180] Loss: 0.0986\n",
      "[Epoch 200] Loss: 0.1085\n",
      "=== Final 200 Epoch Training Complete ===\n",
      "Test Accuracy: 0.8590\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "#############################################\n",
    "# 2) (선택) 원하는 방식으로 train_mask 설정\n",
    "#    - 예: 테스트셋 제외 후 1000개 노드 골라서 train_mask 구성 등\n",
    "#    - 여기서는 간단히 data.train_mask 사용\n",
    "#############################################\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# 3. test_mask가 False인 노드들(= non-test 노드)만 골라낸 인덱스\n",
    "non_test_indices = torch.where(~test_mask)[0]  # ~test_mask: test_mask가 False인 노드\n",
    "\n",
    "# 4. non_test_indices 중에서 랜덤하게 1000개 선택\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1200]]\n",
    "\n",
    "# 5. 새로운 train_mask 생성\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "\n",
    "# 6. data.train_mask에 할당\n",
    "train_mask = new_train_mask\n",
    "\n",
    "# 이제 data.train_mask에는 test set과 겹치지 않는 1000개의 노드만 True로 설정됨\n",
    "print(\"Train set size:\", train_mask.sum().item())\n",
    "print(\"Overlap with test set:\", (train_mask & data.test_mask).sum().item())  # 0이어야 정상\n",
    "\n",
    "#############################################\n",
    "# 3) GCN 모델 정의\n",
    "#############################################\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        # 특정 노드만 반환 (Poisson 시뮬레이션 시 사용)\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 4) 모델 및 옵티마이저 정의\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "#############################################\n",
    "# 5) (시나리오 B) 먼저 Poisson 시뮬레이션 기반 학습\n",
    "#############################################\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 40  # 총 시뮬레이션 시간\n",
    "time_step = 0.1\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        # Poisson 이벤트 발생 & train_mask\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            # 다음 이벤트 시간\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    # 학습\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 6) 이제 200 epoch 학습(풀-배치 방식)\n",
    "#############################################\n",
    "model.train()\n",
    "for epoch in range(1, 201):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))  # 전체 노드에 대한 forward\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=== Final 200 Epoch Training Complete ===\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))  # 모든 노드에 대한 예측\n",
    "        pred = out.argmax(dim=1)  # 가장 높은 확률을 가진 클래스를 예측\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  # 정확히 맞춘 개수\n",
    "        acc = correct / data.test_mask.sum().item()  # 정확도 계산\n",
    "    return acc\n",
    "\n",
    "# 🔹 테스트 실행\n",
    "test_acc = test()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f400d-ea78-4406-9103-07026a43254b",
   "metadata": {},
   "source": [
    "### 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ba909d28-2d47-4991-8e61-d2f42ee819c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 0.2] Poisson Loss: 1.9273\n",
      "[Time 0.3] Poisson Loss: 1.8685\n",
      "[Time 0.4] Poisson Loss: 1.7790\n",
      "[Time 0.5] Poisson Loss: 1.7411\n",
      "[Time 0.6] Poisson Loss: 1.6240\n",
      "[Time 0.7] Poisson Loss: 1.5658\n",
      "[Time 0.8] Poisson Loss: 1.4673\n",
      "[Time 0.9] Poisson Loss: 1.5078\n",
      "[Time 1.0] Poisson Loss: 1.2359\n",
      "[Time 1.1] Poisson Loss: 1.3279\n",
      "[Time 1.2] Poisson Loss: 1.3021\n",
      "[Time 1.3] Poisson Loss: 1.1882\n",
      "[Time 1.4] Poisson Loss: 1.0880\n",
      "[Time 1.5] Poisson Loss: 0.8803\n",
      "[Time 1.6] Poisson Loss: 0.8957\n",
      "[Time 1.7] Poisson Loss: 0.9983\n",
      "[Time 1.8] Poisson Loss: 0.9296\n",
      "[Time 1.9] Poisson Loss: 0.9022\n",
      "[Time 2.0] Poisson Loss: 0.8067\n",
      "[Time 2.1] Poisson Loss: 0.8207\n",
      "[Time 2.2] Poisson Loss: 0.8101\n",
      "[Time 2.3] Poisson Loss: 0.6444\n",
      "[Time 2.4] Poisson Loss: 0.5908\n",
      "[Time 2.5] Poisson Loss: 0.6608\n",
      "[Time 2.6] Poisson Loss: 0.6179\n",
      "[Time 2.7] Poisson Loss: 0.6301\n",
      "[Time 2.8] Poisson Loss: 0.5700\n",
      "[Time 2.9] Poisson Loss: 0.5816\n",
      "[Time 3.0] Poisson Loss: 0.4714\n",
      "[Time 3.1] Poisson Loss: 0.4689\n",
      "[Time 3.2] Poisson Loss: 0.3193\n",
      "[Time 3.3] Poisson Loss: 0.4935\n",
      "[Time 3.4] Poisson Loss: 0.4486\n",
      "[Time 3.5] Poisson Loss: 0.3066\n",
      "[Time 3.6] Poisson Loss: 0.4827\n",
      "[Time 3.7] Poisson Loss: 0.3571\n",
      "[Time 3.8] Poisson Loss: 0.5145\n",
      "[Time 3.9] Poisson Loss: 0.2155\n",
      "[Time 4.0] Poisson Loss: 0.4188\n",
      "[Time 4.1] Poisson Loss: 0.3420\n",
      "[Time 4.2] Poisson Loss: 0.5012\n",
      "[Time 4.3] Poisson Loss: 0.3437\n",
      "[Time 4.4] Poisson Loss: 0.5362\n",
      "[Time 4.5] Poisson Loss: 0.3628\n",
      "[Time 4.6] Poisson Loss: 0.4526\n",
      "[Time 4.7] Poisson Loss: 0.3458\n",
      "[Time 4.8] Poisson Loss: 0.3321\n",
      "[Time 4.9] Poisson Loss: 0.4312\n",
      "[Time 5.0] Poisson Loss: 0.3565\n",
      "[Time 5.1] Poisson Loss: 0.2985\n",
      "[Time 5.2] Poisson Loss: 0.2816\n",
      "[Time 5.3] Poisson Loss: 0.2293\n",
      "[Time 5.4] Poisson Loss: 0.5581\n",
      "[Time 5.5] Poisson Loss: 0.3055\n",
      "[Time 5.6] Poisson Loss: 0.2758\n",
      "[Time 5.7] Poisson Loss: 0.1997\n",
      "[Time 5.8] Poisson Loss: 0.2673\n",
      "[Time 5.9] Poisson Loss: 0.2275\n",
      "[Time 6.0] Poisson Loss: 0.3205\n",
      "[Time 6.1] Poisson Loss: 0.2081\n",
      "[Time 6.2] Poisson Loss: 0.2676\n",
      "[Time 6.3] Poisson Loss: 0.2030\n",
      "[Time 6.4] Poisson Loss: 0.1651\n",
      "[Time 6.5] Poisson Loss: 0.3211\n",
      "[Time 6.6] Poisson Loss: 0.2314\n",
      "[Time 6.7] Poisson Loss: 0.3094\n",
      "[Time 6.8] Poisson Loss: 0.2526\n",
      "[Time 6.9] Poisson Loss: 0.3465\n",
      "[Time 7.0] Poisson Loss: 0.1925\n",
      "[Time 7.1] Poisson Loss: 0.3164\n",
      "[Time 7.2] Poisson Loss: 0.3425\n",
      "[Time 7.3] Poisson Loss: 0.6719\n",
      "[Time 7.4] Poisson Loss: 0.3996\n",
      "[Time 7.5] Poisson Loss: 0.2292\n",
      "[Time 7.6] Poisson Loss: 0.3931\n",
      "[Time 7.7] Poisson Loss: 0.1462\n",
      "[Time 7.8] Poisson Loss: 0.0820\n",
      "[Time 7.9] Poisson Loss: 0.2365\n",
      "[Time 8.0] Poisson Loss: 0.1976\n",
      "[Time 8.1] Poisson Loss: 0.1637\n",
      "[Time 8.2] Poisson Loss: 0.3896\n",
      "[Time 8.3] Poisson Loss: 0.2346\n",
      "[Time 8.4] Poisson Loss: 0.2285\n",
      "[Time 8.5] Poisson Loss: 0.1915\n",
      "[Time 8.6] Poisson Loss: 0.3720\n",
      "[Time 8.7] Poisson Loss: 0.3127\n",
      "[Time 8.8] Poisson Loss: 0.1824\n",
      "[Time 8.9] Poisson Loss: 0.1973\n",
      "[Time 9.0] Poisson Loss: 0.1228\n",
      "[Time 9.1] Poisson Loss: 0.1346\n",
      "[Time 9.2] Poisson Loss: 0.2596\n",
      "[Time 9.3] Poisson Loss: 0.1778\n",
      "[Time 9.4] Poisson Loss: 0.3108\n",
      "[Time 9.5] Poisson Loss: 0.2750\n",
      "[Time 9.6] Poisson Loss: 0.3852\n",
      "[Time 9.7] Poisson Loss: 0.1143\n",
      "[Time 9.8] Poisson Loss: 0.2314\n",
      "[Time 9.9] Poisson Loss: 0.2465\n",
      "[Time 10.0] Poisson Loss: 0.4088\n",
      "[Time 10.1] Poisson Loss: 0.2623\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 000] Loss: 0.2666 | Test Acc: 0.8490\n",
      "[Epoch 010] Loss: 0.1982 | Test Acc: 0.8560\n",
      "[Epoch 020] Loss: 0.1634 | Test Acc: 0.8520\n",
      "[Epoch 030] Loss: 0.1427 | Test Acc: 0.8590\n",
      "[Epoch 040] Loss: 0.1286 | Test Acc: 0.8560\n",
      "[Epoch 050] Loss: 0.1170 | Test Acc: 0.8540\n",
      "[Epoch 060] Loss: 0.1077 | Test Acc: 0.8570\n",
      "[Epoch 070] Loss: 0.0992 | Test Acc: 0.8560\n",
      "[Epoch 080] Loss: 0.0916 | Test Acc: 0.8550\n",
      "[Epoch 090] Loss: 0.0847 | Test Acc: 0.8540\n",
      "[Epoch 100] Loss: 0.0783 | Test Acc: 0.8540\n",
      "[Epoch 110] Loss: 0.0723 | Test Acc: 0.8500\n",
      "[Epoch 120] Loss: 0.0668 | Test Acc: 0.8480\n",
      "[Epoch 130] Loss: 0.0616 | Test Acc: 0.8450\n",
      "[Epoch 140] Loss: 0.0567 | Test Acc: 0.8410\n",
      "[Epoch 150] Loss: 0.0522 | Test Acc: 0.8380\n",
      "[Epoch 160] Loss: 0.0482 | Test Acc: 0.8390\n",
      "[Epoch 170] Loss: 0.0444 | Test Acc: 0.8400\n",
      "[Epoch 180] Loss: 0.0410 | Test Acc: 0.8390\n",
      "[Epoch 190] Loss: 0.0380 | Test Acc: 0.8380\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.8610 (at Epoch 29)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 1. Cora 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ 2. Train Mask 생성 (test_mask와 겹치지 않는 1800개 노드 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1200]]\n",
    "\n",
    "new_train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "new_train_mask[train_indices] = True\n",
    "train_mask = new_train_mask  # 새 train_mask 적용\n",
    "\n",
    "# ✅ 3. 4-layer GCN 모델 정의 (BatchNorm 제거)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim1, hidden_dim2, hidden_dim3, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, hidden_dim3)\n",
    "        self.conv4 = GCNConv(hidden_dim3, out_features)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.conv1.lin.weight)\n",
    "        init.xavier_uniform_(self.conv2.lin.weight)\n",
    "        init.xavier_uniform_(self.conv3.lin.weight)\n",
    "        init.xavier_uniform_(self.conv4.lin.weight)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "# ✅ 4. 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim1=64, \n",
    "    hidden_dim2=32, \n",
    "    hidden_dim3=16, \n",
    "    out_features=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 5. Poisson 시뮬레이션 기반 학습\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 10  # 총 시뮬레이션 시간\n",
    "time_step = 0.1\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "# ✅ 6. 이제 200 epoch 학습 (풀-배치 방식, Best Test Accuracy 저장)\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 Test Accuracy가 가장 높은 순간 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch  # 언제 최고였는지 저장\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193dc05c-0fe0-4b9f-8a39-df715e88eac7",
   "metadata": {},
   "source": [
    "## 2. train from baseline - about 140 train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9e01e0b-7518-445d-8d9c-84da197dc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 0.6] Poisson Loss: 1.9592\n",
      "[Time 0.9] Poisson Loss: 1.8939\n",
      "[Time 1.2] Poisson Loss: 1.8845\n",
      "[Time 1.5] Poisson Loss: 1.7842\n",
      "[Time 1.8] Poisson Loss: 1.8316\n",
      "[Time 2.1] Poisson Loss: 1.5880\n",
      "[Time 2.4] Poisson Loss: 1.5230\n",
      "[Time 2.7] Poisson Loss: 1.5371\n",
      "[Time 3.0] Poisson Loss: 1.3999\n",
      "[Time 3.3] Poisson Loss: 1.3294\n",
      "[Time 3.6] Poisson Loss: 1.1986\n",
      "[Time 3.9] Poisson Loss: 1.1602\n",
      "[Time 4.2] Poisson Loss: 1.2492\n",
      "[Time 4.5] Poisson Loss: 1.0860\n",
      "[Time 4.8] Poisson Loss: 0.9923\n",
      "[Time 5.1] Poisson Loss: 1.0043\n",
      "[Time 5.4] Poisson Loss: 1.0940\n",
      "[Time 5.7] Poisson Loss: 0.9376\n",
      "[Time 6.0] Poisson Loss: 0.7580\n",
      "[Time 6.3] Poisson Loss: 0.7760\n",
      "[Time 6.6] Poisson Loss: 0.9895\n",
      "[Time 6.9] Poisson Loss: 0.8650\n",
      "[Time 7.2] Poisson Loss: 0.6895\n",
      "[Time 7.5] Poisson Loss: 0.6208\n",
      "[Time 7.8] Poisson Loss: 0.5445\n",
      "[Time 8.1] Poisson Loss: 0.7142\n",
      "[Time 8.4] Poisson Loss: 0.8390\n",
      "[Time 8.7] Poisson Loss: 0.6115\n",
      "[Time 9.0] Poisson Loss: 0.5700\n",
      "[Time 9.3] Poisson Loss: 0.5443\n",
      "[Time 9.6] Poisson Loss: 0.4532\n",
      "[Time 9.9] Poisson Loss: 0.3582\n",
      "[Time 10.2] Poisson Loss: 0.4093\n",
      "[Time 10.5] Poisson Loss: 0.2111\n",
      "[Time 10.8] Poisson Loss: 0.3797\n",
      "[Time 11.1] Poisson Loss: 0.4631\n",
      "[Time 11.4] Poisson Loss: 0.3603\n",
      "[Time 11.7] Poisson Loss: 0.4345\n",
      "[Time 12.0] Poisson Loss: 0.2560\n",
      "[Time 12.3] Poisson Loss: 0.1788\n",
      "[Time 12.6] Poisson Loss: 0.2662\n",
      "[Time 12.9] Poisson Loss: 0.3464\n",
      "[Time 13.2] Poisson Loss: 0.2166\n",
      "[Time 13.5] Poisson Loss: 0.3635\n",
      "[Time 13.8] Poisson Loss: 0.2421\n",
      "[Time 14.1] Poisson Loss: 0.4492\n",
      "[Time 14.4] Poisson Loss: 0.2488\n",
      "[Time 14.7] Poisson Loss: 0.1404\n",
      "[Time 15.0] Poisson Loss: 0.1181\n",
      "[Time 15.3] Poisson Loss: 0.1059\n",
      "[Time 15.6] Poisson Loss: 0.0747\n",
      "[Time 15.9] Poisson Loss: 0.2721\n",
      "[Time 16.2] Poisson Loss: 0.2260\n",
      "[Time 16.5] Poisson Loss: 0.0892\n",
      "[Time 16.8] Poisson Loss: 0.1258\n",
      "[Time 17.1] Poisson Loss: 0.2410\n",
      "[Time 17.4] Poisson Loss: 0.2519\n",
      "[Time 17.7] Poisson Loss: 0.0975\n",
      "[Time 18.0] Poisson Loss: 0.1102\n",
      "[Time 18.3] Poisson Loss: 0.1088\n",
      "[Time 18.6] Poisson Loss: 0.0668\n",
      "[Time 18.9] Poisson Loss: 0.0940\n",
      "[Time 19.2] Poisson Loss: 0.2267\n",
      "[Time 19.5] Poisson Loss: 0.2122\n",
      "[Time 19.8] Poisson Loss: 0.0886\n",
      "[Time 20.1] Poisson Loss: 0.1298\n",
      "[Time 20.4] Poisson Loss: 0.1434\n",
      "[Time 20.7] Poisson Loss: 0.1308\n",
      "[Time 21.0] Poisson Loss: 0.1335\n",
      "[Time 21.3] Poisson Loss: 0.1328\n",
      "[Time 21.6] Poisson Loss: 0.0817\n",
      "[Time 21.9] Poisson Loss: 0.1072\n",
      "[Time 22.2] Poisson Loss: 0.1559\n",
      "[Time 22.5] Poisson Loss: 0.0674\n",
      "[Time 22.8] Poisson Loss: 0.0524\n",
      "[Time 23.1] Poisson Loss: 0.0571\n",
      "[Time 23.4] Poisson Loss: 0.1550\n",
      "[Time 23.7] Poisson Loss: 0.1978\n",
      "[Time 24.0] Poisson Loss: 0.0660\n",
      "[Time 24.3] Poisson Loss: 0.0159\n",
      "[Time 24.6] Poisson Loss: 0.0867\n",
      "[Time 24.9] Poisson Loss: 0.0978\n",
      "[Time 25.2] Poisson Loss: 0.1308\n",
      "[Time 25.5] Poisson Loss: 0.1006\n",
      "[Time 25.8] Poisson Loss: 0.0285\n",
      "[Time 26.1] Poisson Loss: 0.0652\n",
      "[Time 26.4] Poisson Loss: 0.1977\n",
      "[Time 26.7] Poisson Loss: 0.0664\n",
      "[Time 27.0] Poisson Loss: 0.0376\n",
      "[Time 27.3] Poisson Loss: 0.2137\n",
      "[Time 27.6] Poisson Loss: 0.1018\n",
      "[Time 27.9] Poisson Loss: 0.0828\n",
      "[Time 28.2] Poisson Loss: 0.1233\n",
      "[Time 28.5] Poisson Loss: 0.0461\n",
      "[Time 28.8] Poisson Loss: 0.0501\n",
      "[Time 29.1] Poisson Loss: 0.1253\n",
      "[Time 29.4] Poisson Loss: 0.0382\n",
      "[Time 29.7] Poisson Loss: 0.1499\n",
      "[Time 30.0] Poisson Loss: 0.0809\n",
      "[Time 30.3] Poisson Loss: 0.0918\n",
      "[Time 30.6] Poisson Loss: 0.1010\n",
      "[Time 30.9] Poisson Loss: 0.0841\n",
      "[Time 31.2] Poisson Loss: 0.0644\n",
      "[Time 31.5] Poisson Loss: 0.0736\n",
      "[Time 31.8] Poisson Loss: 0.1035\n",
      "[Time 32.1] Poisson Loss: 0.0445\n",
      "[Time 32.4] Poisson Loss: 0.1133\n",
      "[Time 32.7] Poisson Loss: 0.0472\n",
      "[Time 33.0] Poisson Loss: 0.0850\n",
      "[Time 33.3] Poisson Loss: 0.1206\n",
      "[Time 33.6] Poisson Loss: 0.0554\n",
      "[Time 33.9] Poisson Loss: 0.0329\n",
      "[Time 34.2] Poisson Loss: 0.0577\n",
      "[Time 34.5] Poisson Loss: 0.0695\n",
      "[Time 34.8] Poisson Loss: 0.0443\n",
      "[Time 35.1] Poisson Loss: 0.0459\n",
      "[Time 35.4] Poisson Loss: 0.0878\n",
      "[Time 35.7] Poisson Loss: 0.0795\n",
      "[Time 36.0] Poisson Loss: 0.0765\n",
      "[Time 36.3] Poisson Loss: 0.0771\n",
      "[Time 36.6] Poisson Loss: 0.0637\n",
      "[Time 36.9] Poisson Loss: 0.0650\n",
      "[Time 37.2] Poisson Loss: 0.0598\n",
      "[Time 37.5] Poisson Loss: 0.0569\n",
      "[Time 37.8] Poisson Loss: 0.0410\n",
      "[Time 38.1] Poisson Loss: 0.0761\n",
      "[Time 38.4] Poisson Loss: 0.1398\n",
      "[Time 38.7] Poisson Loss: 0.1318\n",
      "[Time 39.0] Poisson Loss: 0.0614\n",
      "[Time 39.3] Poisson Loss: 0.0520\n",
      "[Time 39.6] Poisson Loss: 0.0221\n",
      "[Time 39.9] Poisson Loss: 0.0939\n",
      "[Time 40.2] Poisson Loss: 0.0491\n",
      "[Time 40.5] Poisson Loss: 0.1138\n",
      "[Time 40.8] Poisson Loss: 0.0519\n",
      "[Time 41.1] Poisson Loss: 0.1033\n",
      "[Time 41.4] Poisson Loss: 0.0479\n",
      "[Time 41.7] Poisson Loss: 0.0606\n",
      "[Time 42.0] Poisson Loss: 0.0749\n",
      "[Time 42.3] Poisson Loss: 0.0101\n",
      "[Time 42.6] Poisson Loss: 0.0277\n",
      "[Time 42.9] Poisson Loss: 0.0330\n",
      "[Time 43.2] Poisson Loss: 0.0146\n",
      "[Time 43.5] Poisson Loss: 0.2211\n",
      "[Time 43.8] Poisson Loss: 0.0431\n",
      "[Time 44.1] Poisson Loss: 0.0364\n",
      "[Time 44.4] Poisson Loss: 0.0557\n",
      "[Time 44.7] Poisson Loss: 0.0606\n",
      "[Time 45.0] Poisson Loss: 0.0208\n",
      "[Time 45.3] Poisson Loss: 0.0636\n",
      "[Time 45.6] Poisson Loss: 0.0447\n",
      "[Time 45.9] Poisson Loss: 0.0423\n",
      "[Time 46.2] Poisson Loss: 0.0301\n",
      "[Time 46.5] Poisson Loss: 0.0151\n",
      "[Time 46.8] Poisson Loss: 0.0420\n",
      "[Time 47.1] Poisson Loss: 0.0316\n",
      "[Time 47.4] Poisson Loss: 0.0775\n",
      "[Time 47.7] Poisson Loss: 0.0211\n",
      "[Time 48.0] Poisson Loss: 0.0213\n",
      "[Time 48.3] Poisson Loss: 0.0489\n",
      "[Time 48.6] Poisson Loss: 0.0420\n",
      "[Time 48.9] Poisson Loss: 0.0369\n",
      "[Time 49.2] Poisson Loss: 0.0759\n",
      "[Time 49.5] Poisson Loss: 0.0577\n",
      "[Time 49.8] Poisson Loss: 0.0131\n",
      "[Time 50.1] Poisson Loss: 0.0237\n",
      "[Time 50.4] Poisson Loss: 0.0254\n",
      "[Time 50.7] Poisson Loss: 0.0339\n",
      "[Time 51.0] Poisson Loss: 0.0489\n",
      "[Time 51.3] Poisson Loss: 0.0805\n",
      "[Time 51.6] Poisson Loss: 0.0149\n",
      "[Time 51.9] Poisson Loss: 0.1249\n",
      "[Time 52.2] Poisson Loss: 0.0650\n",
      "[Time 52.5] Poisson Loss: 0.0459\n",
      "[Time 52.8] Poisson Loss: 0.0199\n",
      "[Time 53.1] Poisson Loss: 0.0637\n",
      "[Time 53.4] Poisson Loss: 0.0460\n",
      "[Time 53.7] Poisson Loss: 0.0425\n",
      "[Time 54.0] Poisson Loss: 0.0276\n",
      "[Time 54.3] Poisson Loss: 0.0567\n",
      "[Time 54.6] Poisson Loss: 0.0582\n",
      "[Time 54.9] Poisson Loss: 0.0833\n",
      "[Time 55.2] Poisson Loss: 0.0722\n",
      "[Time 55.5] Poisson Loss: 0.0374\n",
      "[Time 55.8] Poisson Loss: 0.0072\n",
      "[Time 56.1] Poisson Loss: 0.0306\n",
      "[Time 56.4] Poisson Loss: 0.0279\n",
      "[Time 56.7] Poisson Loss: 0.0141\n",
      "[Time 57.0] Poisson Loss: 0.0181\n",
      "[Time 57.3] Poisson Loss: 0.0205\n",
      "[Time 57.6] Poisson Loss: 0.0145\n",
      "[Time 57.9] Poisson Loss: 0.1024\n",
      "[Time 58.2] Poisson Loss: 0.0588\n",
      "[Time 58.5] Poisson Loss: 0.0551\n",
      "[Time 58.8] Poisson Loss: 0.0884\n",
      "[Time 59.1] Poisson Loss: 0.0372\n",
      "[Time 59.4] Poisson Loss: 0.0477\n",
      "[Time 59.7] Poisson Loss: 0.0398\n",
      "[Time 60.0] Poisson Loss: 0.0491\n",
      "[Time 60.3] Poisson Loss: 0.0422\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 020] Loss: 0.0361\n",
      "[Epoch 040] Loss: 0.0334\n",
      "[Epoch 060] Loss: 0.0352\n",
      "[Epoch 080] Loss: 0.0586\n",
      "[Epoch 100] Loss: 0.0309\n",
      "[Epoch 120] Loss: 0.0343\n",
      "[Epoch 140] Loss: 0.0273\n",
      "[Epoch 160] Loss: 0.0360\n",
      "[Epoch 180] Loss: 0.0209\n",
      "[Epoch 200] Loss: 0.0211\n",
      "=== Final 200 Epoch Training Complete ===\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/', name='cora')\n",
    "data = dataset[0]\n",
    "\n",
    "#############################################\n",
    "# 2) (선택) 원하는 방식으로 train_mask 설정\n",
    "#    - 예: 테스트셋 제외 후 1000개 노드 골라서 train_mask 구성 등\n",
    "#    - 여기서는 간단히 data.train_mask 사용\n",
    "#############################################\n",
    "train_mask = data.train_mask\n",
    "\n",
    "#############################################\n",
    "# 3) GCN 모델 정의\n",
    "#############################################\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        # 특정 노드만 반환 (Poisson 시뮬레이션 시 사용)\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 4) 모델 및 옵티마이저 정의\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "#############################################\n",
    "# 5) (시나리오 B) 먼저 Poisson 시뮬레이션 기반 학습\n",
    "#############################################\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 60  # 총 시뮬레이션 시간\n",
    "time_step = 0.3\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.to(device), selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        # Poisson 이벤트 발생 & train_mask\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            # 다음 이벤트 시간\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    # 학습\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 6) 이제 200 epoch 학습(풀-배치 방식)\n",
    "#############################################\n",
    "model.train()\n",
    "for epoch in range(1, 201):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.to(device))  # 전체 노드에 대한 forward\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=== Final 200 Epoch Training Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85bdfa92-8679-4d78-a6cd-27042d17bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8160\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.to(device))  # 모든 노드에 대한 예측\n",
    "        pred = out.argmax(dim=1)  # 가장 높은 확률을 가진 클래스를 예측\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  # 정확히 맞춘 개수\n",
    "        acc = correct / data.test_mask.sum().item()  # 정확도 계산\n",
    "    return acc\n",
    "\n",
    "# 🔹 테스트 실행\n",
    "test_acc = test()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ea3a0-8f32-4467-a976-0fce9eda3ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed13ade-6f99-409c-b02c-568eb44ce5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f8abe58-5d88-46a2-ae69-35072d26a45c",
   "metadata": {},
   "source": [
    "# 2. Oversmoothing 관련\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d32a31a-3c1d-4a74-a858-4a7896de2596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 0.2] Poisson Loss: 1.7918\n",
      "[Time 0.3] Poisson Loss: 1.7894\n",
      "[Time 0.4] Poisson Loss: 1.7833\n",
      "[Time 0.5] Poisson Loss: 1.7752\n",
      "[Time 0.6] Poisson Loss: 1.7608\n",
      "[Time 0.7] Poisson Loss: 1.7322\n",
      "[Time 0.8] Poisson Loss: 1.6344\n",
      "[Time 0.9] Poisson Loss: 1.5654\n",
      "[Time 1.0] Poisson Loss: 1.6932\n",
      "[Time 1.1] Poisson Loss: 1.5506\n",
      "[Time 1.2] Poisson Loss: 1.6668\n",
      "[Time 1.3] Poisson Loss: 1.5483\n",
      "[Time 1.4] Poisson Loss: 1.5329\n",
      "[Time 1.5] Poisson Loss: 1.6460\n",
      "[Time 1.6] Poisson Loss: 1.4995\n",
      "[Time 1.7] Poisson Loss: 1.5557\n",
      "[Time 1.8] Poisson Loss: 1.4877\n",
      "[Time 1.9] Poisson Loss: 1.5330\n",
      "[Time 2.0] Poisson Loss: 1.4929\n",
      "[Time 2.1] Poisson Loss: 1.6411\n",
      "[Time 2.2] Poisson Loss: 1.5707\n",
      "[Time 2.3] Poisson Loss: 1.5276\n",
      "[Time 2.4] Poisson Loss: 1.4577\n",
      "[Time 2.5] Poisson Loss: 1.4895\n",
      "[Time 2.6] Poisson Loss: 1.4629\n",
      "[Time 2.7] Poisson Loss: 1.4908\n",
      "[Time 2.8] Poisson Loss: 1.5677\n",
      "[Time 2.9] Poisson Loss: 1.4704\n",
      "[Time 3.0] Poisson Loss: 1.5152\n",
      "[Time 3.1] Poisson Loss: 1.5553\n",
      "[Time 3.2] Poisson Loss: 1.4585\n",
      "[Time 3.3] Poisson Loss: 1.5885\n",
      "[Time 3.4] Poisson Loss: 1.4422\n",
      "[Time 3.5] Poisson Loss: 1.4152\n",
      "[Time 3.6] Poisson Loss: 1.5109\n",
      "[Time 3.7] Poisson Loss: 1.4314\n",
      "[Time 3.8] Poisson Loss: 1.4369\n",
      "[Time 3.9] Poisson Loss: 1.3840\n",
      "[Time 4.0] Poisson Loss: 1.4483\n",
      "[Time 4.1] Poisson Loss: 1.4651\n",
      "[Time 4.2] Poisson Loss: 1.5035\n",
      "[Time 4.3] Poisson Loss: 1.4856\n",
      "[Time 4.4] Poisson Loss: 1.4785\n",
      "[Time 4.5] Poisson Loss: 1.4933\n",
      "[Time 4.6] Poisson Loss: 1.5038\n",
      "[Time 4.7] Poisson Loss: 1.2727\n",
      "[Time 4.8] Poisson Loss: 1.4738\n",
      "[Time 4.9] Poisson Loss: 1.4128\n",
      "[Time 5.0] Poisson Loss: 1.2628\n",
      "[Time 5.1] Poisson Loss: 1.3309\n",
      "[Time 5.2] Poisson Loss: 1.5004\n",
      "[Time 5.3] Poisson Loss: 1.3628\n",
      "[Time 5.4] Poisson Loss: 1.3425\n",
      "[Time 5.5] Poisson Loss: 1.3077\n",
      "[Time 5.6] Poisson Loss: 1.3340\n",
      "[Time 5.7] Poisson Loss: 1.2593\n",
      "[Time 5.8] Poisson Loss: 1.3829\n",
      "[Time 5.9] Poisson Loss: 1.4866\n",
      "[Time 6.0] Poisson Loss: 1.3519\n",
      "[Time 6.1] Poisson Loss: 1.2702\n",
      "=== Poisson-based Training Complete ===\n",
      "[Epoch 000] Loss: 1.3409 | Test Acc: 0.4240\n",
      "[Epoch 010] Loss: 1.2820 | Test Acc: 0.4400\n",
      "[Epoch 020] Loss: 1.2172 | Test Acc: 0.4590\n",
      "[Epoch 030] Loss: 1.1562 | Test Acc: 0.4800\n",
      "[Epoch 040] Loss: 1.0487 | Test Acc: 0.6300\n",
      "[Epoch 050] Loss: 0.8949 | Test Acc: 0.6420\n",
      "[Epoch 060] Loss: 0.8197 | Test Acc: 0.6770\n",
      "[Epoch 070] Loss: 0.7711 | Test Acc: 0.6590\n",
      "[Epoch 080] Loss: 0.7401 | Test Acc: 0.6820\n",
      "[Epoch 090] Loss: 0.7293 | Test Acc: 0.6860\n",
      "[Epoch 100] Loss: 0.7343 | Test Acc: 0.6760\n",
      "[Epoch 110] Loss: 0.6995 | Test Acc: 0.6930\n",
      "[Epoch 120] Loss: 0.6850 | Test Acc: 0.6890\n",
      "[Epoch 130] Loss: 0.6704 | Test Acc: 0.6990\n",
      "[Epoch 140] Loss: 0.6559 | Test Acc: 0.7110\n",
      "[Epoch 150] Loss: 0.6296 | Test Acc: 0.7000\n",
      "[Epoch 160] Loss: 0.6255 | Test Acc: 0.7070\n",
      "[Epoch 170] Loss: 0.6321 | Test Acc: 0.6990\n",
      "[Epoch 180] Loss: 0.5878 | Test Acc: 0.7120\n",
      "[Epoch 190] Loss: 0.5850 | Test Acc: 0.7050\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.7160 (at Epoch 184)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 1. Citeseer 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ 2. Train Mask 생성 (test_mask와 겹치지 않는 1800개 노드 선택)\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "\n",
    "# ✅ 3. 16-layer GCN 모델 (Residual Connection 없이 그냥 쌓음)\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, num_layers=32):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # 첫 번째 레이어\n",
    "        self.convs.append(GCNConv(in_features, hidden_dim))\n",
    "        \n",
    "        # 중간 레이어 (전부 hidden_dim 유지)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # 마지막 레이어\n",
    "        self.convs.append(GCNConv(hidden_dim, out_features))\n",
    "        \n",
    "    def forward(self, data, selected_nodes=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:  # 마지막 레이어는 활성화 X\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        if selected_nodes is not None and len(selected_nodes) > 0:\n",
    "            selected_nodes = torch.tensor(selected_nodes, dtype=torch.long, device=x.device)\n",
    "            return x[selected_nodes]  \n",
    "        return x\n",
    "\n",
    "# ✅ 4. 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepGCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim=64, \n",
    "    out_features=dataset.num_classes,\n",
    "    num_layers=16\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 5. Poisson 시뮬레이션 기반 학습\n",
    "N = data.num_nodes\n",
    "lambda_ = 0.5\n",
    "T = 6  # 총 시뮬레이션 시간\n",
    "time_step = 0.1\n",
    "\n",
    "# 각 노드별 첫 이벤트 시간 (지수분포)\n",
    "event_times = [np.random.exponential(1/lambda_) for _ in range(N)]\n",
    "event_logs = [[] for _ in range(N)]\n",
    "\n",
    "# numpy 형태의 train_mask (True/False)\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "\n",
    "def train_poisson(selected_nodes):\n",
    "    if len(selected_nodes) == 0:\n",
    "        return None\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data, selected_nodes)\n",
    "    labels = data.y[selected_nodes].to(device)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ===== Poisson 시뮬레이션 루프 =====\n",
    "model.train()\n",
    "t = 0\n",
    "while t < T:\n",
    "    selected_nodes = []\n",
    "    for i in range(N):\n",
    "        if t >= event_times[i] and train_mask_np[i]:\n",
    "            selected_nodes.append(i)\n",
    "            event_logs[i].append(t)\n",
    "            event_times[i] += np.random.exponential(1/lambda_)\n",
    "\n",
    "    t += time_step\n",
    "\n",
    "    if selected_nodes:\n",
    "        loss_val = train_poisson(selected_nodes)\n",
    "        if loss_val is not None:\n",
    "            print(f\"[Time {t:.1f}] Poisson Loss: {loss_val:.4f}\")\n",
    "\n",
    "print(\"=== Poisson-based Training Complete ===\")\n",
    "\n",
    "# ✅ 6. 이제 200 epoch 학습 (풀-배치 방식, Best Test Accuracy 저장)\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 Test Accuracy가 가장 높은 순간 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51112261-2660-4a17-9d9c-39af9be7b77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (3703) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], data\u001b[38;5;241m.\u001b[39my[train_mask])\n\u001b[1;32m     76\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m, in \u001b[0;36mDeepGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# 🔹 Residual Connection (매 2개 레이어마다 추가)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midentity\u001b[49m  \u001b[38;5;66;03m# 원본 x를 더함\u001b[39;00m\n\u001b[1;32m     49\u001b[0m             identity \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# 업데이트된 x를 새로운 identity로 저장\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (3703) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ Train Mask 생성\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "\n",
    "# ✅ 16-layer GCN 모델 (Residual Connection 포함)\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, num_layers=16):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # 첫 번째 레이어\n",
    "        self.convs.append(GCNConv(in_features, hidden_dim))\n",
    "        \n",
    "        # 중간 레이어 (Residual 적용)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # 마지막 레이어\n",
    "        self.convs.append(GCNConv(hidden_dim, out_features))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        identity = x  # Residual을 위해 원본 x 저장\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            \n",
    "            if i < self.num_layers - 1:  # 마지막 레이어에서는 활성화 X\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.2, training=self.training)\n",
    "                \n",
    "                # 🔹 Residual Connection (매 2개 레이어마다 추가)\n",
    "                if i % 2 == 1:\n",
    "                    x = x + identity  # 원본 x를 더함\n",
    "                    identity = x  # 업데이트된 x를 새로운 identity로 저장\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ✅ 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepGCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim=64, \n",
    "    out_features=dataset.num_classes,\n",
    "    num_layers=16  # 16개 레이어 사용\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 학습 루프\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 🔹 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 최고 정확도 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfca0f9d-f130-4958-8cab-af3087798144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] Loss: 1.7918 | Test Acc: 0.2310\n",
      "[Epoch 010] Loss: 1.6079 | Test Acc: 0.2960\n",
      "[Epoch 020] Loss: 1.4406 | Test Acc: 0.3860\n",
      "[Epoch 030] Loss: 1.3153 | Test Acc: 0.4280\n",
      "[Epoch 040] Loss: 1.2483 | Test Acc: 0.4500\n",
      "[Epoch 050] Loss: 1.1886 | Test Acc: 0.4470\n",
      "[Epoch 060] Loss: 1.1633 | Test Acc: 0.5200\n",
      "[Epoch 070] Loss: 1.0384 | Test Acc: 0.5950\n",
      "[Epoch 080] Loss: 0.9452 | Test Acc: 0.6010\n",
      "[Epoch 090] Loss: 0.8413 | Test Acc: 0.6980\n",
      "[Epoch 100] Loss: 0.7850 | Test Acc: 0.7040\n",
      "[Epoch 110] Loss: 0.7250 | Test Acc: 0.6940\n",
      "[Epoch 120] Loss: 0.6812 | Test Acc: 0.7190\n",
      "[Epoch 130] Loss: 0.6799 | Test Acc: 0.7040\n",
      "[Epoch 140] Loss: 0.6682 | Test Acc: 0.7130\n",
      "[Epoch 150] Loss: 0.6560 | Test Acc: 0.7240\n",
      "[Epoch 160] Loss: 0.6376 | Test Acc: 0.7200\n",
      "[Epoch 170] Loss: 0.6370 | Test Acc: 0.7300\n",
      "[Epoch 180] Loss: 0.6272 | Test Acc: 0.7250\n",
      "[Epoch 190] Loss: 0.6131 | Test Acc: 0.7340\n",
      "=== Training Complete ===\n",
      "Best Test Accuracy: 0.7400 (at Epoch 176)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# ✅ 데이터셋 로드\n",
    "dataset = Planetoid(root='data/', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# ✅ Train Mask 생성\n",
    "test_mask = data.test_mask\n",
    "non_test_indices = torch.where(~test_mask)[0]\n",
    "train_indices = non_test_indices[torch.randperm(len(non_test_indices))[:1800]]\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "\n",
    "# ✅ 16-layer GCN 모델 (Residual Connection 없음)\n",
    "# class DeepGCN(torch.nn.Module):\n",
    "#     def __init__(self, in_features, hidden_dim, out_features, num_layers=64):\n",
    "#         super().__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "#         # 첫 번째 레이어\n",
    "#         self.convs.append(GCNConv(in_features, hidden_dim))\n",
    "        \n",
    "#         # 중간 14개 레이어 (모두 hidden_dim 유지)\n",
    "#         for _ in range(num_layers - 2):\n",
    "#             self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "#         # 마지막 레이어\n",
    "#         self.convs.append(GCNConv(hidden_dim, out_features))\n",
    "        \n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "\n",
    "#         for i, conv in enumerate(self.convs):\n",
    "#             x = conv(x, edge_index)\n",
    "#             if i < self.num_layers - 1:  # 마지막 레이어는 활성화 X\n",
    "#                 x = F.relu(x)\n",
    "#                 x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, num_layers=64):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        # self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # 첫 번째 레이어\n",
    "        modules = []\n",
    "        modules.extend([GCNConv(in_features, hidden_dim), nn.ReLU()])\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.append([GCNConv(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        modules.append(GCNConv(hidden_dim, out_features)\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "    # https://pytorch-geometric.readthedocs.io/en/2.5.1/modules/nn.html 참고\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:  # 마지막 레이어는 활성화 X\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ✅ 모델 및 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepGCN(\n",
    "    in_features=dataset.num_features, \n",
    "    hidden_dim=64, \n",
    "    out_features=dataset.num_classes,\n",
    "    num_layers=16\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# ✅ 학습 루프\n",
    "best_test_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 🔹 테스트 수행\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "    # 🔹 최고 정확도 저장\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch:03d}] Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"=== Training Complete ===\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (at Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65854ff-4940-403d-bad0-2db2cd8cf605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b6537-3073-4c27-b50a-32d6ba4c04b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myENV_kernel",
   "language": "python",
   "name": "myenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
